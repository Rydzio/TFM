class(nombres$doc_name)
df <- data.frame("term" = nombres$doc_name, "TF_IDF"= tfdf$tfidf)
df
library(tidyverse)
toCompare <- statPOSSplit %>% select(keyword)
toCompare <- unique(toCompare)
nrow(toCompare)
newCol = c()
for (keyword_name in statPOSSplit$keyword) {
append(newCol,tfidf[keyword_name])
}
nrow(newCol)
newCol = c()
for (keyword_name in statPOSSplit$keyword) {
#append(newCol,tfidf[keyword_name])
print(tfidf[keyword_name])
}
append(newCol, "2")
(newCol)
(newCol, "2")
append(newCol, "2")
append(newCol, "2")
append(newCol, "2") -> a
newCol = c()
for (keyword_name in statPOSSplit$keyword) {
append(newCol,tfidf[keyword_name]) -> newCol
}
newCol = c()
for (keyword_name in statPOSSplit$keyword) {
append(newCol,tfidf[keyword_name]) -> newCol
}
library(readtext)
hilos = 8
#font_import()
#Ruta de trabajo
setwd("~/TFM")
ruta = "/doc"
#ruta = "/Users/pedrohv/TFM/TerminologíasInteractivas/data/covid19/raw/documents"
#Leer un corpus
docs <- readtext(paste0(getwd(),ruta, "*"), #Leo todo lo que tenga ese path
#docvarsfrom = "filenames",
#docvarnames = c("document", "language"),
#dvsep = "_",
encoding = "UTF-8-BOM", #"ISO-8859-1", #Casi mejor no pongo nada porque no sÃ© el encoding
verbosity = 3)
print("Se han leido los documentos del corpus con éxito")
library(quanteda)
# create quanteda corpus
quanteda_options(threads = hilos)
quancorpusDocs <- corpus(docs)
#Obtenemos un resumen del corpus que hemops creado
summ <- summary(quancorpusDocs,    #Esto tarda unos segundos. Types es el num de tokens Únicos.
n = nrow(docs))    #Por defecto son 100
sum(summ$Sentences)
sum(summ$Tokens)
#Puedo sacar los textos
tDocs <- texts(quancorpusDocs) #No tarda nada.
#Un vector nombrado (cada elemento tiene el nombre del doc).
#Cada elemento es una cadena con el texto del doc.
library(udpipe)
library(tictoc)
model <- udpipe_download_model(language = "spanish")
#udmodel_spanish_gsd <- udpipe_load_model(file = 'spanish-gsd-ud-2.4-190531.udpipe')
path <- model$file_model
tic()
x <- udpipe(tDocs, path, parallel.cores = hilos)
toc()
saveRDS(x, file = "termExtraction/udpipeEspañol1.rds")
#x <- readRDS(file = "airbus.rds")
library(lattice)
#Plotting Part-of-speech tags from the given text
stats <- txt_freq(x$upos) #upos = universal part of speech
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = stats, col = "yellow",
main = "UPOS (Universal Parts of Speech)\n frequency of occurrence",
xlab = "Freq")
stats <- subset(x, upos %in% "NOUN")
stats <- txt_freq(x = stats$lemma)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 30), col = "cadetblue", main = "Most occurring nouns", xlab = "Freq")
stats <- subset(x, upos %in% c("VERB"))
stats <- txt_freq(stats$token)# tambiÃ©n puedo poner lemma si quiero ver el verbo en infinitivo
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "gold",
main = "Most occurring Verbs", xlab = "Freq")
stats <- subset(x, upos %in% c("VERB"))
stats <- txt_freq(stats$lemma)# el lemma para el verbo en infinitivo
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "gold",
main = "Most occurring Verbs (lemma)", xlab = "Freq")
## Collocation (words following one another)
stats <- keywords_collocation(x = x,
term = "token", group = c("doc_id", "paragraph_id", "sentence_id"),
ngram_max = 4)
## Co-occurrences: How frequent do words occur in the same sentence, in this case only nouns or adjectives
stats <- cooccurrence(x = subset(x, upos %in% c("NOUN", "ADJ")),
term = "lemma", group = c("doc_id", "paragraph_id", "sentence_id"))
## Co-occurrences: How frequent do words follow one another
stats <- cooccurrence(x = x$lemma,
relevant = x$upos %in% c("NOUN", "ADJ"))
## Co-occurrences: How frequent do words follow one another even if we would skip 2 words in between
stats <- cooccurrence(x = x$lemma,
relevant = x$upos %in% c("NOUN", "ADJ"), skipgram = 2)
head(stats)
library(textrank)
stats <- textrank_keywords(x$lemma,
relevant = x$upos %in% c("NOUN", "ADJ"),
ngram_max = 8, sep = " ")
stats <- subset(stats$keywords, ngram > 1 & freq >= 5)
stats
TermFreq <- document_term_frequencies(x)
stats <- keywords_rake(x = x,
term = "lemma",
group = "doc_id",
relevant = x$upos %in% c("NOUN", "ADJ"))
stats$key <- factor(stats$keyword, levels = rev(stats$keyword))
stats
x$phrase_tag <- as_phrasemachine(x$upos,
type = "upos" #Puede ser tambiÃ©n "penn-treebank"
)#Convierte los tags de upos a phrasemachine (Handler 2016).
statsPOS <- keywords_phrases(x = x$phrase_tag,
term = tolower(x$token),
pattern = "(A|N)*N(P+D*(A|N)*N)*",
is_regex = TRUE,
detailed = TRUE
)
statsPOS
# Para cada doc_id calcular el keyword_phrases y añadirle una columna con su doc_id
split(x, x$doc_id) -> xSplit
statsPOS2 <- data.frame()
statPOSSplit <- data.frame()
for (doc_id in xSplit) {
statsPOS <- keywords_phrases(x = doc_id$phrase_tag,
term = tolower(doc_id$token),
pattern = "(A|N)*N(P+D*(A|N)*N)*",
is_regex = TRUE,
detailed = FALSE
)
cbind(rep(doc_id$doc_id[1], nrow(statsPOS)), statsPOS) ->> statsPOS2
colnames(statsPOS2)[1] <- "doc_id"
rbind(statsPOS2, statPOSSplit) ->> statPOSSplit
}
statPOSSplit
TestingFreq <- statPOSSplit[, c("doc_id", "keyword", "freq")]
colnames(TestingFreq)[2] <- "term"
dtm <- document_term_matrix(TestingFreq)
## Calculate tfidf
tfidf <- dtm_tfidf(dtm)
tfdf <- data.frame(tfidf)
nombres <- data.frame(dimnames(tfidf))
colnames(nombres)[1] <- "doc_name"
#cbind(nombres, tfdf$tfidf) -> df
#df
tfdf$tfidf <- as.character(tfdf$tfidf)
tfdf
nombres$doc_name <- as.character(nombres$doc_name)
class(nombres$doc_name)
df <- data.frame("term" = nombres$doc_name, "TF_IDF"= tfdf$tfidf)
df
library(tidyverse)
toCompare <- statPOSSplit %>% select(keyword)
toCompare <- unique(toCompare)
nrow(toCompare)
newCol = c()
for (keyword_name in statPOSSplit$keyword) {
append(newCol,tfidf[keyword_name]) -> newCol
}
nrow(newCol)
newCol = c()
for (keyword_name in statPOSSplit$keyword) {
append(newCol,tfidf[keyword_name]) ->> newCol
}
nrow(newCol)
newCol = c()
for (keyword_name in statPOSSplit$keyword) {
append(newCol,tfidf[keyword_name]) ->> newCol
print(newCol)
}
newCol = c()
for (keyword_name in statPOSSplit$keyword) {
append(newCol,tfidf[keyword_name]) ->> newCol
}
length(newCol)
newcol <- data.frame(newCol)
View(newcol)
View(newcol)
newCol = c()
for (keyword_name in statPOSSplit$keyword) {
append(newCol,tfidf[keyword_name]) ->> newCol
}
length(newCol)
cbind(statPOSSplit, newClol) -> statsTF_IDF
cbind(statPOSSplit, newCol) -> statsTF_IDF
cbind(statPOSSplit, newCol) -> statsTF_IDF
statsTF_IDF
cbind(statPOSSplit, newCol) -> statsTF_IDF
colnames(statsTF_IDF)[5] <- "TF_IDF"
statsTF_IDF
View(statsTF_IDF)
library(readtext)
hilos = 8
#font_import()
#Ruta de trabajo
setwd("~/TFM")
#ruta = "/doc"
ruta = "/legal"
#ruta = "/Users/pedrohv/TFM/TerminologíasInteractivas/data/covid19/raw/documents"
#Leer un corpus
docs <- readtext(paste0(getwd(),ruta, "*"), #Leo todo lo que tenga ese path
#docvarsfrom = "filenames",
#docvarnames = c("document", "language"),
#dvsep = "_",
encoding = "UTF-8-BOM", #"ISO-8859-1", #Casi mejor no pongo nada porque no sÃ© el encoding
verbosity = 3)
print("Se han leido los documentos del corpus con éxito")
library(readtext)
hilos = 8
#font_import()
#Ruta de trabajo
setwd("~/TFM")
#ruta = "/doc"
ruta = "/legal"
#ruta = "/Users/pedrohv/TFM/TerminologíasInteractivas/data/covid19/raw/documents"
#Leer un corpus
docs <- readtext(paste0(getwd(),ruta, "*"), #Leo todo lo que tenga ese path
#docvarsfrom = "filenames",
#docvarnames = c("document", "language"),
#dvsep = "_",
encoding = "UTF-8-BOM", #"ISO-8859-1", #Casi mejor no pongo nada porque no sÃ© el encoding
verbosity = 3)
print("Se han leido los documentos del corpus con éxito")
library(quanteda)
# create quanteda corpus
quanteda_options(threads = hilos)
quancorpusDocs <- corpus(docs)
#Obtenemos un resumen del corpus que hemops creado
summ <- summary(quancorpusDocs,    #Esto tarda unos segundos. Types es el num de tokens Únicos.
n = nrow(docs))    #Por defecto son 100
sum(summ$Sentences)
sum(summ$Tokens)
#Puedo sacar los textos
tDocs <- texts(quancorpusDocs) #No tarda nada.
#Un vector nombrado (cada elemento tiene el nombre del doc).
#Cada elemento es una cadena con el texto del doc.
library(udpipe)
library(tictoc)
model <- udpipe_download_model(language = "spanish")
#udmodel_spanish_gsd <- udpipe_load_model(file = 'spanish-gsd-ud-2.4-190531.udpipe')
path <- model$file_model
tic()
x <- udpipe(tDocs, path, parallel.cores = hilos)
toc()
saveRDS(x, file = "termExtraction/udpipeEspañol1.rds")
#x <- readRDS(file = "airbus.rds")
library(lattice)
#Plotting Part-of-speech tags from the given text
stats <- txt_freq(x$upos) #upos = universal part of speech
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = stats, col = "yellow",
main = "UPOS (Universal Parts of Speech)\n frequency of occurrence",
xlab = "Freq")
stats <- subset(x, upos %in% "NOUN")
stats <- txt_freq(x = stats$lemma)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 30), col = "cadetblue", main = "Most occurring nouns", xlab = "Freq")
stats <- subset(x, upos %in% c("VERB"))
stats <- txt_freq(stats$token)# tambiÃ©n puedo poner lemma si quiero ver el verbo en infinitivo
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "gold",
main = "Most occurring Verbs", xlab = "Freq")
stats <- subset(x, upos %in% c("VERB"))
stats <- txt_freq(stats$lemma)# el lemma para el verbo en infinitivo
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "gold",
main = "Most occurring Verbs (lemma)", xlab = "Freq")
## Collocation (words following one another)
stats <- keywords_collocation(x = x,
term = "token", group = c("doc_id", "paragraph_id", "sentence_id"),
ngram_max = 4)
## Co-occurrences: How frequent do words occur in the same sentence, in this case only nouns or adjectives
stats <- cooccurrence(x = subset(x, upos %in% c("NOUN", "ADJ")),
term = "lemma", group = c("doc_id", "paragraph_id", "sentence_id"))
## Co-occurrences: How frequent do words follow one another
stats <- cooccurrence(x = x$lemma,
relevant = x$upos %in% c("NOUN", "ADJ"))
## Co-occurrences: How frequent do words follow one another even if we would skip 2 words in between
stats <- cooccurrence(x = x$lemma,
relevant = x$upos %in% c("NOUN", "ADJ"), skipgram = 2)
head(stats)
library(textrank)
stats <- textrank_keywords(x$lemma,
relevant = x$upos %in% c("NOUN", "ADJ"),
ngram_max = 8, sep = " ")
stats <- subset(stats$keywords, ngram > 1 & freq >= 5)
stats
TermFreq <- document_term_frequencies(x)
stats <- keywords_rake(x = x,
term = "lemma",
group = "doc_id",
relevant = x$upos %in% c("NOUN", "ADJ"))
stats$key <- factor(stats$keyword, levels = rev(stats$keyword))
stats
x$phrase_tag <- as_phrasemachine(x$upos,
type = "upos" #Puede ser tambiÃ©n "penn-treebank"
)#Convierte los tags de upos a phrasemachine (Handler 2016).
statsPOS <- keywords_phrases(x = x$phrase_tag,
term = tolower(x$token),
pattern = "(A|N)*N(P+D*(A|N)*N)*",
is_regex = TRUE,
detailed = TRUE
)
statsPOS
# Para cada doc_id calcular el keyword_phrases y añadirle una columna con su doc_id
split(x, x$doc_id) -> xSplit
statsPOS2 <- data.frame()
statPOSSplit <- data.frame()
for (doc_id in xSplit) {
statsPOS <- keywords_phrases(x = doc_id$phrase_tag,
term = tolower(doc_id$token),
pattern = "(A|N)*N(P+D*(A|N)*N)*",
is_regex = TRUE,
detailed = FALSE
)
cbind(rep(doc_id$doc_id[1], nrow(statsPOS)), statsPOS) ->> statsPOS2
colnames(statsPOS2)[1] <- "doc_id"
rbind(statsPOS2, statPOSSplit) ->> statPOSSplit
}
statPOSSplit
TestingFreq <- statPOSSplit[, c("doc_id", "keyword", "freq")]
colnames(TestingFreq)[2] <- "term"
dtm <- document_term_matrix(TestingFreq)
## Calculate tfidf
tfidf <- dtm_tfidf(dtm)
tfdf <- data.frame(tfidf)
nombres <- data.frame(dimnames(tfidf))
colnames(nombres)[1] <- "doc_name"
#cbind(nombres, tfdf$tfidf) -> df
#df
tfdf$tfidf <- as.character(tfdf$tfidf)
tfdf
nombres$doc_name <- as.character(nombres$doc_name)
class(nombres$doc_name)
df <- data.frame("term" = nombres$doc_name, "TF_IDF"= tfdf$tfidf)
df
library(tidyverse)
toCompare <- statPOSSplit %>% select(keyword)
toCompare <- unique(toCompare)
nrow(toCompare)
newCol = c()
for (keyword_name in statPOSSplit$keyword) {
append(newCol,tfidf[keyword_name]) ->> newCol
}
length(newCol)
cbind(statPOSSplit, newCol) -> statsTF_IDF
colnames(statsTF_IDF)[5] <- "TF_IDF"
statsTF_IDF
library(readtext)
hilos = 8
#font_import()
#Ruta de trabajo
setwd("~/TFM")
ruta = "/doc"
#ruta = "/legal"
#ruta = "/Users/pedrohv/TFM/TerminologíasInteractivas/data/covid19/raw/documents"
#Leer un corpus
docs <- readtext(paste0(getwd(),ruta, "*"), #Leo todo lo que tenga ese path
#docvarsfrom = "filenames",
#docvarnames = c("document", "language"),
#dvsep = "_",
encoding = "UTF-8-BOM", #"ISO-8859-1", #Casi mejor no pongo nada porque no sÃ© el encoding
verbosity = 3)
print("Se han leido los documentos del corpus con éxito")
library(quanteda)
# create quanteda corpus
quanteda_options(threads = hilos)
quancorpusDocs <- corpus(docs)
#Obtenemos un resumen del corpus que hemops creado
summ <- summary(quancorpusDocs,    #Esto tarda unos segundos. Types es el num de tokens Únicos.
n = nrow(docs))    #Por defecto son 100
sum(summ$Sentences)
sum(summ$Tokens)
#Puedo sacar los textos
tDocs <- texts(quancorpusDocs) #No tarda nada.
#Un vector nombrado (cada elemento tiene el nombre del doc).
#Cada elemento es una cadena con el texto del doc.
library(udpipe)
library(tictoc)
model <- udpipe_download_model(language = "spanish")
#udmodel_spanish_gsd <- udpipe_load_model(file = 'spanish-gsd-ud-2.4-190531.udpipe')
path <- model$file_model
tic()
x <- udpipe(tDocs, path, parallel.cores = hilos)
library(readtext)
library(tictoc)
hilos = 8
#font_import()
#Ruta de trabajo
setwd("~/TFM")
ruta = "/doc"
#ruta = "/legal"
#ruta = "/Users/pedrohv/TFM/TerminologíasInteractivas/data/covid19/raw/documents"
#Leer un corpus
tic()
docs <- readtext(paste0(getwd(),ruta, "*"), #Leo todo lo que tenga ese path
#docvarsfrom = "filenames",
#docvarnames = c("document", "language"),
#dvsep = "_",
encoding = "UTF-8-BOM", #"ISO-8859-1", #Casi mejor no pongo nada porque no sÃ© el encoding
verbosity = 3)
toc()
print("Se han leido los documentos del corpus con éxito")
library(quanteda)
tic()
# create quanteda corpus
quanteda_options(threads = hilos)
quancorpusDocs <- corpus(docs)
#Obtenemos un resumen del corpus que hemops creado
summ <- summary(quancorpusDocs,    #Esto tarda unos segundos. Types es el num de tokens Únicos.
n = nrow(docs))    #Por defecto son 100
sum(summ$Sentences)
sum(summ$Tokens)
#Puedo sacar los textos
tDocs <- texts(quancorpusDocs) #No tarda nada.
#Un vector nombrado (cada elemento tiene el nombre del doc).
#Cada elemento es una cadena con el texto del doc.
toc()
library(udpipe)
model <- udpipe_download_model(language = "spanish")
path <- model$file_model
tic()
x <- udpipe(tDocs, path, parallel.cores = hilos)
toc()
x$phrase_tag <- as_phrasemachine(x$upos,
type = "upos" #Puede ser tambiÃ©n "penn-treebank"
)#Convierte los tags de upos a phrasemachine (Handler 2016).
tic()
statsPOS <- keywords_phrases(x = x$phrase_tag,
term = tolower(x$token),
pattern = "(A|N)*N(P+D*(A|N)*N)*",
is_regex = TRUE,
detailed = FALSE
)
toc()
statsPOS
library(dplyr)
x$phrase_tag <- as_phrasemachine(x$upos,
type = "upos" #Puede ser tambiÃ©n "penn-treebank"
)#Convierte los tags de upos a phrasemachine (Handler 2016).
tic()
statsPOS2 <- data.frame()
statsPOSSplit <- data.frame()
split(x, x$doc_id) -> xSplit
for (doc_id in xSplit) {
statsPOS <- keywords_phrases(x = doc_id$phrase_tag,
term = tolower(doc_id$lemma),
pattern = "(A|N)*N(P+D*(A|N)*N)*",
is_regex = TRUE,
detailed = FALSE
)
cbind(rep(doc_id$doc_id[1], nrow(statsPOS)), statsPOS) ->> statsPOS2
colnames(statsPOS2)[1] <- "doc_id"
rbind(statsPOS2, statsPOSSplit) ->> statsPOSSplit
}
toc()
statsPOSSplit
total_words <- statsPOSSplit %>%
group_by(doc_id) %>%
summarize(total = sum(freq))
statsPOSSplit <- left_join(statsPOSSplit, total_words)
statsPOSSplit
library(ggplot2)
ggplot(statsPOSSplit, aes(freq/total, fill = doc_id)) +
geom_histogram(show.legend = FALSE) +
xlim(NA, 0.0009) +
facet_wrap(~doc_id, ncol = 4, scales = "free_y")
freq_by_rank <- statsPOSSplit %>%
group_by(doc_id) %>%
mutate(rank = row_number(),
`term frequency` = freq/total)
freq_by_rank
freq_by_rank %>%
ggplot(aes(rank, `term frequency`, color = doc_id)) +
geom_abline(intercept = -0.62, slope = -1.1, color = "gray50", linetype = 2) +
geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
scale_x_log10() +
scale_y_log10()
library(tidytext)
statsPOSSplit <- statsPOSSplit %>%
bind_tf_idf(keyword, doc_id, freq)
statsPOSSplit
statsPOSSplit %>%
arrange(desc(tf_idf)) %>%
mutate(keyword = factor(keyword, levels = rev(unique(keyword)))) %>%
group_by(doc_id) %>%
top_n(15) %>%
ungroup() %>%
ggplot(aes(keyword, tf_idf, fill = doc_id)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~doc_id, ncol = 2, scales = "free") +
coord_flip()
statsPOSSplit %>%
arrange(desc(tf_idf)) %>%
mutate(keyword = factor(keyword, levels = rev(unique(keyword)))) %>%
group_by(doc_id) %>%
top_n(15) %>%
ungroup() %>%
ggplot(aes(keyword, tf_idf, fill = doc_id)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~doc_id, ncol = 2, scales = "free") +
coord_flip()

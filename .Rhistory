scale_y_continuous(labels = scales::percent_format(accuracy = 1),  #Que ponga porcentaje (en lugar de float) en eje y
breaks = seq(0,1,0.1), #Que pinte estos (de 0 a 1 (100%) de 0.1 (10%) en 10%)
expand = expand_scale(mult = c(0, 0)) #Elimina espacios extra por debajo y por arriba, para que vaya justo de 0 a 1 (100%).
) +
labs( #title = "The title",
#subtitle = "the subtitle",        #will be displayed below the title.
#caption = "Some ref of copyright" #The text for the caption which will be displayed in the bottom-right of the plot by default.
#tag = "some letter useful "
x = "POS patterns ordered (most used in the left)", #Label of the x axis
y = "Percentage of entities covered (cummulative)" #Label of the x axis
) +
geom_bar(stat="identity",
show.legend = FALSE, #Avoids leyend (big, one class per color)
#color = "#111111", #Color del borde. Si no lo pongo no poner borde
#fill = defcolor   #Color del relleno
) +
scale_fill_manual (values = micolor, #coloreo distinto EL INTERIOR de las top barras
limits = cumdf$pat #Es imprescindible si estoy forzando el orden
) +
geom_segment(aes(x=cumdf$pat[1],  y=sumtop  , xend=cumdf$pat[top], yend=sumtop),   #Add a horizontal segment (line)
size = 0.5, #Grosor de la línea. Por def = 1
color="red") +
geom_segment(aes(x=cumdf$pat[top], y=0.0,      xend=cumdf$pat[top], yend=sumtop),   #Add a vertical segment (line)
size = 0.5, #Grosor de la línea. Por def = 1
color="red") +
annotate(geom="text",
x=cumdf$pat[1],
y=sumtop + 0.05, #A litte above the line
hjust = 0, #La posición será la del inicio de la cadena
label=paste0("The ", top, " most used patterns cover ",
round(100* max(cumsum(tab_ord[1:top])/nrow(x)), #Top x cumsum
digits=0 #Sin decimales
),
"% of the terms"
),
color="red") +
theme_classic() + #Light es un theme algo más mono que bw o classic pero que impreso no queda bien
theme(#element_text(size=20),                            #El tamaño del font general
axis.text.x = element_text(angle = 90,#Para girar las labels del eje x
#size = rel(0.95), # puedo poner un vaor absoluto (e.g. 10) o relativo
vjust = 0.5, #0 means left-justified, 1 means right-justified.
hjust = 1    #0 means left-justified, 1 means right-justified.
)
)
library(ggplot2)
tabsum <- cumsum(tab_ord)/nrow(x)
cumdf <- data.frame(cum = tabsum,
stringsAsFactors = FALSE) #Tiene una única col
cumdf$pat <- rownames(cumdf) #Nueva col con pattens
top <- 12
defcolor <- "#EEEEEE"  #light gray para imprimir (casi no se ve en pantalla). Si uso #BBBBBB sale muy oscuro impreso
#Un tono más oscuro es #CCCCCC . Si uso #777777 sale oscurito en pantalla pero muy oscuro impreso
micolor <- c(rep("#CCCCCC", top), rep(defcolor, nrow(cumdf) - top)) #Vector de colores
sumtop <- cumdf$cum[top] #Suma de los 12 primeros
ggplot(data=cumdf,
aes(x=pat, y=cum, fill = pat)) +
scale_x_discrete(limits = cumdf$pat) + #Sin esto ordena alfabéticamente
scale_y_continuous(labels = scales::percent_format(accuracy = 1),  #Que ponga porcentaje (en lugar de float) en eje y
breaks = seq(0,1,0.1), #Que pinte estos (de 0 a 1 (100%) de 0.1 (10%) en 10%)
expand = expand_scale(mult = c(0, 0)) #Elimina espacios extra por debajo y por arriba, para que vaya justo de 0 a 1 (100%).
) +
labs( #title = "The title",
#subtitle = "the subtitle",        #will be displayed below the title.
#caption = "Some ref of copyright" #The text for the caption which will be displayed in the bottom-right of the plot by default.
#tag = "some letter useful "
x = "UPOS patterns ordered (most used in the left)", #Label of the x axis
y = "Percentage of entities covered (cummulative)" #Label of the x axis
) +
geom_bar(stat="identity",
show.legend = FALSE, #Avoids leyend (big, one class per color)
#color = "#111111", #Color del borde. Si no lo pongo no poner borde
#fill = defcolor   #Color del relleno
) +
scale_fill_manual (values = micolor, #coloreo distinto EL INTERIOR de las top barras
limits = cumdf$pat #Es imprescindible si estoy forzando el orden
) +
geom_segment(aes(x=cumdf$pat[1],  y=sumtop  , xend=cumdf$pat[top], yend=sumtop),   #Add a horizontal segment (line)
size = 0.5, #Grosor de la línea. Por def = 1
color="red") +
geom_segment(aes(x=cumdf$pat[top], y=0.0,      xend=cumdf$pat[top], yend=sumtop),   #Add a vertical segment (line)
size = 0.5, #Grosor de la línea. Por def = 1
color="red") +
annotate(geom="text",
x=cumdf$pat[1],
y=sumtop + 0.05, #A litte above the line
hjust = 0, #La posición será la del inicio de la cadena
label=paste0("The ", top, " most used UPOS cover ",
round(100* max(cumsum(tab_ord[1:top])/nrow(x)), #Top x cumsum
digits=0 #Sin decimales
),
"% of the terms"
),
color="red") +
theme_classic() + #Light es un theme algo más mono que bw o classic pero que impreso no queda bien
theme(#element_text(size=20),                            #El tamaño del font general
axis.text.x = element_text(angle = 90,#Para girar las labels del eje x
#size = rel(0.95), # puedo poner un vaor absoluto (e.g. 10) o relativo
vjust = 0.5, #0 means left-justified, 1 means right-justified.
hjust = 1    #0 means left-justified, 1 means right-justified.
)
)
library(dplyr)
library(plyr)
cleanStats <- subset(stats2, select = c(keyword, pattern, ngram))
#Así, podemos obtener la frecuencia de aparicion de un termino específico
frequency <- ddply(cleanStats, .(keyword, pattern, ngram), nrow)
#Así, podemos obtener la frecuencia de aparicion de un termino específico
frequencyOfPOS <- ddply(cleanStats, .(pattern, ngram), nrow)
#Así, podemos obtener la frecuencia de aparicion de un termino específico
frequencyOfUPOS <- ddply(x, .(upos), nrow)
frequencyOfUPOS
setwd("~/NLP/Linx/Spanish")
#Leo termcat_collective_negotiation_es.csv
df <- read.csv("termcat_collective_negotiation_es.csv" ,
header=FALSE,
blank.lines.skip = FALSE, #Fundamental!!!! Sin esto no lee líneas vacias
stringsAsFactors = FALSE
)
#Elimino las primeras 9 líneas (cabeceras)
df <- df[-c(1:9),]
#Tiene tres columnas (la última está vacía) RARO!
#Renombro columnas
colnames(df) <- c("uri", "label", "V3")
#which(df$V3 != "")
#Dice que todas las celdas de la tercera columna están vacías.
# create quanteda corpus
library(quanteda)
quanteda_options(threads = 10)
quancorpusDocs <- corpus(df$label)#Cada label es un documento
summ <- summary(quancorpusDocs,    #Esto tarda unos segundos. Types es el num de tokens únicos.
n = length(df$label))#Por defecto son 100
#sum(summ$Sentences) #Salen 719
#sum(summ$Tokens)    #Salen 1718
#Puedo sacar los textos
tDocs <- texts(quancorpusDocs) #No tarda nada.
#Un vector nombrado (cada elemento tiene el nombre del doc).
#Cada elemento es una cadena con el texto del doc.
#Reemplazo en t todas las apariciones de los \00X por ""
library(stringr)
#tmp= c("\001\004AB\006", "AB\005CD\001")
t2  <- str_replace_all(tDocs,
c("\001" = "",
"\002" = "",
"\003" = "",
"\004" = "",
"\005" = "",
"\006" = ""
)
)
library(udpipe)
#model <- udpipe_download_model(language = "spanish-ancora") #Only first time to get the .udpipe file
#Alternativas: (1) spanish
#              (2) spanish-ancora
#fileWithPath <- paste0(getwd(), "/", params$model)
udmodel_spanish <- udpipe_load_model(file = params$model) #Lo cojo de la cabecera de Markdown
#Alternativas: (1) spanish-gsd-ud-2.3-181115.udpipe
#              (2) spanish-ancora-ud-2.3-181115.udpipe
s <- udpipe_annotate(udmodel_spanish,
#tokenizer = "tokenizer". El que tiene udpipes por defecto. Otras opciones en http://ufal.mff.cuni.cz/udpipe/users-manual
#tagger = "default".  El POS taggin y lematización por defecto de udpipes. También puede ser "none" o lo que dice en http://ufal.mff.cuni.cz/udpipe/users-manual
#parser = "default". El dependency parsing que tiene udpipes por defecto. También puede ser "none" o lo que dice en http://ufal.mff.cuni.cz/udpipe/users-manual
#trace = TRUE,#Por defecto es FALSE. Muestra el progreso de la anotación.
t2) #Tarda nada
#Se podría PARALELIZAR. Mira https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-parallel.html
x <- data.frame(s) #Tarda unos minutos
#save(x, file="x.df.udpipeanno.termcat.es.saved") #Lo cargo con load("x.df.udpipeanno.saved")
#load("x.df.udpipeanno.termcat.es.saved")
#Plotting Part-of-speech tags from the given text
library(lattice)
stats <- txt_freq(x$upos) #upos = universal part of speech
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = stats, col = "yellow",
main = "UPOS (Universal Parts of Speech)\n frequency of occurrence",
xlab = "Freq")
stats <- subset(x, upos %in% c("VERB"))
stats <- txt_freq(stats$token)# también puedo poner lemma si quiero ver el verbo en infinitivo
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "gold",
main = "Most occurring Verbs (token)", xlab = "Freq")
stats <- subset(x, upos %in% c("VERB"))
stats <- txt_freq(stats$lemma)# el lemma para el verbo en infinitivo
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "light blue",
main = "Most occurring Verbs (lemma)", xlab = "Freq")
x$phrase_tag <- as_phrasemachine(x$upos,
type = "upos" #Puede ser también "penn-treebank"
)#Convierte los tags de upos a phrasemachine (Handler 2016).
df_doc_token     <- aggregate(token ~ doc_id, data = x, paste, collapse = " ")
df_doc_phrase_tag <- aggregate(phrase_tag ~ doc_id, data = x, paste, collapse = "")
df_doc_token_phrasetag <- df_doc_token
df_doc_token_phrasetag$phrase_tag <- df_doc_phrase_tag$phrase_tag
tab <- table(df_doc_token_phrasetag$phrase_tag)
#Ordeno la tabla de contingencia
tab_ord <- sort(tab, decreasing = T)
#Lo que más hay es NA (216), luego N (141)....
library(knitr)
kable (tab_ord, caption = "Número de casos de cada POS pattern") #Muestro todos
barplot(tab_ord,
log="y", #Log scale in y axis
main="POS patterns",
ylab="Number of instances (log scale)",
xlab="POS pattern",
las=2 #Rotated lables in the x axis
)
barplot(cumsum(tab_ord)/nrow(df_doc_token_phrasetag),
#log="y", #Log scale in y axis
main="POS patterns (Cumulative Sum)",
ylab="Percentage covered",
xlab="POS patterns",
las=2 #Rotated lables in the x axis
)
library(ggplot2)
tabsum <- cumsum(tab_ord)/nrow(df_doc_token_phrasetag)
cumdf <- data.frame(cum = tabsum,
stringsAsFactors = FALSE) #Tiene una única col
cumdf$pat <- rownames(cumdf) #Nueva col con pattens
top <- 12
defcolor <- "#EEEEEE"  #light gray para imprimir (casi no se ve en pantalla). Si uso #BBBBBB sale muy oscuro impreso
#Un tono más oscuro es #CCCCCC . Si uso #777777 sale oscurito en pantalla pero muy oscuro impreso
micolor <- c(rep("#CCCCCC", top), rep(defcolor, nrow(cumdf) - top)) #Vector de colores
sumtop <- cumdf$cum[top] #Suma de los 12 primeros
ggplot(data=cumdf,
aes(x=pat, y=cum, fill = pat)) +
scale_x_discrete(limits = cumdf$pat) + #Sin esto ordena alfabéticamente
scale_y_continuous(labels = scales::percent_format(accuracy = 1),  #Que ponga porcentaje (en lugar de float) en eje y
breaks = seq(0,1,0.1), #Que pinte estos (de 0 a 1 (100%) de 0.1 (10%) en 10%)
expand = expand_scale(mult = c(0, 0)) #Elimina espacios extra por debajo y por arriba, para que vaya justo de 0 a 1 (100%).
) +
labs( #title = "The title",
#subtitle = "the subtitle",        #will be displayed below the title.
#caption = "Some ref of copyright" #The text for the caption which will be displayed in the bottom-right of the plot by default.
#tag = "some letter useful "
x = "POS patterns ordered (most used in the left)", #Label of the x axis
y = "Percentage of entities covered (cummulative)" #Label of the x axis
) +
geom_bar(stat="identity",
show.legend = FALSE, #Avoids leyend (big, one class per color)
#color = "#111111", #Color del borde. Si no lo pongo no poner borde
#fill = defcolor   #Color del relleno
) +
scale_fill_manual (values = micolor, #coloreo distinto EL INTERIOR de las top barras
limits = cumdf$pat #Es imprescindible si estoy forzando el orden
) +
geom_segment(aes(x=cumdf$pat[1],  y=sumtop  , xend=cumdf$pat[top], yend=sumtop),   #Add a horizontal segment (line)
size = 0.5, #Grosor de la línea. Por def = 1
color="red") +
geom_segment(aes(x=cumdf$pat[top], y=0.0,      xend=cumdf$pat[top], yend=sumtop),   #Add a vertical segment (line)
size = 0.5, #Grosor de la línea. Por def = 1
color="red") +
annotate(geom="text",
x=cumdf$pat[1],
y=sumtop + 0.05, #A litte above the line
hjust = 0, #La posición será la del inicio de la cadena
label=paste0("The ", top, " most used patterns cover ",
round(100* max(cumsum(tab_ord[1:top])/nrow(df_doc_token_phrasetag)), #Top x cumsum
digits=0 #Sin decimales
),
"% of the terms"
),
color="red") +
theme_classic() + #Light es un theme algo más mono que bw o classic pero que impreso no queda bien
theme(#element_text(size=20),                            #El tamaño del font general
axis.text.x = element_text(angle = 90,#Para girar las labels del eje x
#size = rel(0.95), # puedo poner un vaor absoluto (e.g. 10) o relativo
vjust = 0.5, #0 means left-justified, 1 means right-justified.
hjust = 1    #0 means left-justified, 1 means right-justified.
)
)
#Veamos un caso
library(knitr)
#Sale que con NPNPN hay 18 términos
ejemplosPat <- function(pat){
df_doc_token_phrasetag[df_doc_token_phrasetag$phrase_tag == pat, ]
} #retorna un data.frame
kable (ejemplosPat("NPNPN"), caption = "Todos los términos con el POS pattern NPNPN") #Muestro todos
#Los ejemplos de pattern ("NA, "AN", "NPNA", etc...) hasta un máximo de 20
tablaEjemplosPat <- function(pat){
ej <- ejemplosPat(pat)
num <- min(20, nrow(ej))
kable (ej[c(1:num),], caption = paste (num, "términos con el POS pattern", pat))
}
# tablaEjemplosPat("NA")
# tablaEjemplosPat("N")
# tablaEjemplosPat("NPN")
# tablaEjemplosPat("V")
# tablaEjemplosPat("NPNA")
# tablaEjemplosPat("NPDN")
# tablaEjemplosPat("A")
# tablaEjemplosPat("AN")
# tablaEjemplosPat("NN")
# tablaEjemplosPat("NPNPDNPNAA")
# tablaEjemplosPat("NOPDNAO")
# tablaEjemplosPat("O")
# tablaEjemplosPat("NPNOPDNAO")
# tablaEjemplosPat("NAA")
# tablaEjemplosPat("NAPN")
for  (pat in cumdf$pat){
tablaEjemplosPat(pat)
}
library(readtext)
hilos = 8
#font_import()
#Ruta de trabajo
setwd("~/TFM")
ruta = "/legalcorpuses*"
#Leer un corpus
docs <- readtext(paste0(getwd(), ruta, "*"), #Leo todo lo que tenga ese path
#docvarsfrom = "filenames",
#docvarnames = c("document", "language"),
#dvsep = "_",
encoding = "UTF-8-BOM", #"ISO-8859-1", #Casi mejor no pongo nada porque no sÃ© el encoding
verbosity = 0)
print("Se han leido los documentos del corpus con éxito")
library(quanteda)
# create quanteda corpus
quanteda_options(threads = hilos)
quancorpusDocs <- corpus(docs)
#Obtenemos un resumen del corpus que hemops creado
summ <- summary(quancorpusDocs,    #Esto tarda unos segundos. Types es el num de tokens Únicos.
n = nrow(docs))    #Por defecto son 100
sum(summ$Sentences)
sum(summ$Tokens)
#Puedo sacar los textos
tDocs <- texts(quancorpusDocs) #No tarda nada.
#Un vector nombrado (cada elemento tiene el nombre del doc).
#Cada elemento es una cadena con el texto del doc.
library(udpipe)
library(tictoc)
model <- udpipe_download_model(language = "spanish")
#udmodel_spanish_gsd <- udpipe_load_model(file = 'spanish-gsd-ud-2.4-190531.udpipe')
path <- model$file_model
tic()
x <- udpipe(tDocs, path, parallel.cores = hilos)
toc()
saveRDS(x, file = "termExtraction/udpipeEspañol1.rds")
#x <- readRDS(file = "airbus.rds")
library(lattice)
#Plotting Part-of-speech tags from the given text
stats <- txt_freq(x$upos) #upos = universal part of speech
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = stats, col = "yellow",
main = "UPOS (Universal Parts of Speech)\n frequency of occurrence",
xlab = "Freq")
stats <- subset(x, upos %in% "NOUN")
stats <- txt_freq(x = stats$lemma)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 30), col = "cadetblue", main = "Most occurring nouns", xlab = "Freq")
stats <- subset(x, upos %in% c("VERB"))
stats <- txt_freq(stats$token)# tambiÃ©n puedo poner lemma si quiero ver el verbo en infinitivo
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "gold",
main = "Most occurring Verbs", xlab = "Freq")
stats <- subset(x, upos %in% c("VERB"))
stats <- txt_freq(stats$lemma)# el lemma para el verbo en infinitivo
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "gold",
main = "Most occurring Verbs (lemma)", xlab = "Freq")
## Collocation (words following one another)
stats <- keywords_collocation(x = x,
term = "token", group = c("doc_id", "paragraph_id", "sentence_id"),
ngram_max = 4)
## Co-occurrences: How frequent do words occur in the same sentence, in this case only nouns or adjectives
stats <- cooccurrence(x = subset(x, upos %in% c("NOUN", "ADJ")),
term = "lemma", group = c("doc_id", "paragraph_id", "sentence_id"))
## Co-occurrences: How frequent do words follow one another
stats <- cooccurrence(x = x$lemma,
relevant = x$upos %in% c("NOUN", "ADJ"))
## Co-occurrences: How frequent do words follow one another even if we would skip 2 words in between
stats <- cooccurrence(x = x$lemma,
relevant = x$upos %in% c("NOUN", "ADJ"), skipgram = 2)
head(stats)
library(igraph)
library(ggraph)
library(ggplot2)
library(extrafont)
wordnetwork <- head(stats, 30)
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "pink") +
geom_node_text(aes(label = name), col = "darkgreen", size = 4) +
theme_graph(base_family = "Arial Narrow") +
theme(legend.position = "none") +
labs(title = "Cooccurrences within 3 words distance", subtitle = "Nouns & Adjective")
library(textrank)
stats <- textrank_keywords(x$lemma,
relevant = x$upos %in% c("NOUN", "ADJ"),
ngram_max = 8, sep = " ")
stats <- subset(stats$keywords, ngram > 1 & freq >= 5)
library(wordcloud)
suppressWarnings(wordcloud(words = stats$keyword, freq = stats$freq))
stats <- keywords_rake(x = x,
term = "lemma",
group = "doc_id",
relevant = x$upos %in% c("NOUN", "ADJ"))
stats$key <- factor(stats$keyword, levels = rev(stats$keyword))
barchart(key ~ rake,
data = head(subset(stats, freq > 20), 60),
col = "red",
main = "Keywords identified by RAKE",
xlab = "Rake")
library(dplyr)
library(plyr)
cleanStats <- subset(stats2, select = c(keyword, pattern, ngram))
#Así, podemos obtener la frecuencia de aparicion de un termino específico
frequency <- ddply(cleanStats, .(keyword, pattern, ngram), nrow)
#Así, podemos obtener la frecuencia de aparicion de un termino específico
frequencyOfPOS <- ddply(cleanStats, .(pattern, ngram), nrow)
#Así, podemos obtener la frecuencia de aparicion de un termino específico
frequencyOfUPOS <- ddply(x, .(upos), nrow)
frequencyOfUPOS
tab <- table(x$upos)
tab_ord <- sort(tab, decreasing = T)
barplot(cumsum(tab_ord)/nrow(x),
#log="y", #Log scale in y axis
main="UPOS patterns (Cumulative Sum)",
ylab="Percentage covered",
xlab="POS patterns",
las=2 #Rotated lables in the x axis
)
library(ggplot2)
tabsum <- cumsum(tab_ord)/nrow(x)
cumdf <- data.frame(cum = tabsum,
stringsAsFactors = FALSE) #Tiene una única col
cumdf$pat <- rownames(cumdf) #Nueva col con pattens
top <- 12
defcolor <- "#EEEEEE"  #light gray para imprimir (casi no se ve en pantalla). Si uso #BBBBBB sale muy oscuro impreso
#Un tono más oscuro es #CCCCCC . Si uso #777777 sale oscurito en pantalla pero muy oscuro impreso
micolor <- c(rep("#CCCCCC", top), rep(defcolor, nrow(cumdf) - top)) #Vector de colores
sumtop <- cumdf$cum[top] #Suma de los 12 primeros
ggplot(data=cumdf,
aes(x=pat, y=cum, fill = pat)) +
scale_x_discrete(limits = cumdf$pat) + #Sin esto ordena alfabéticamente
scale_y_continuous(labels = scales::percent_format(accuracy = 1),  #Que ponga porcentaje (en lugar de float) en eje y
breaks = seq(0,1,0.1), #Que pinte estos (de 0 a 1 (100%) de 0.1 (10%) en 10%)
expand = expand_scale(mult = c(0, 0)) #Elimina espacios extra por debajo y por arriba, para que vaya justo de 0 a 1 (100%).
) +
labs( #title = "The title",
#subtitle = "the subtitle",        #will be displayed below the title.
#caption = "Some ref of copyright" #The text for the caption which will be displayed in the bottom-right of the plot by default.
#tag = "some letter useful "
x = "UPOS patterns ordered (most used in the left)", #Label of the x axis
y = "Percentage of entities covered (cummulative)" #Label of the x axis
) +
geom_bar(stat="identity",
show.legend = FALSE, #Avoids leyend (big, one class per color)
#color = "#111111", #Color del borde. Si no lo pongo no poner borde
#fill = defcolor   #Color del relleno
) +
scale_fill_manual (values = micolor, #coloreo distinto EL INTERIOR de las top barras
limits = cumdf$pat #Es imprescindible si estoy forzando el orden
) +
geom_segment(aes(x=cumdf$pat[1],  y=sumtop  , xend=cumdf$pat[top], yend=sumtop),   #Add a horizontal segment (line)
size = 0.5, #Grosor de la línea. Por def = 1
color="red") +
geom_segment(aes(x=cumdf$pat[top], y=0.0,      xend=cumdf$pat[top], yend=sumtop),   #Add a vertical segment (line)
size = 0.5, #Grosor de la línea. Por def = 1
color="red") +
annotate(geom="text",
x=cumdf$pat[1],
y=sumtop + 0.05, #A litte above the line
hjust = 0, #La posición será la del inicio de la cadena
label=paste0("The ", top, " most used UPOS cover ",
round(100* max(cumsum(tab_ord[1:top])/nrow(x)), #Top x cumsum
digits=0 #Sin decimales
),
"% of the terms"
),
color="red") +
theme_classic() + #Light es un theme algo más mono que bw o classic pero que impreso no queda bien
theme(#element_text(size=20),                            #El tamaño del font general
axis.text.x = element_text(angle = 90,#Para girar las labels del eje x
#size = rel(0.95), # puedo poner un vaor absoluto (e.g. 10) o relativo
vjust = 0.5, #0 means left-justified, 1 means right-justified.
hjust = 1    #0 means left-justified, 1 means right-justified.
)
)
x$phrase_tag <- as_phrasemachine(x$upos,
type = "upos" #Puede ser tambiÃ©n "penn-treebank"
)#Convierte los tags de upos a phrasemachine (Handler 2016).
stats2 <- keywords_phrases(x = x$phrase_tag,
term = tolower(x$token),
pattern = "(A|N)*N(P+D*(A|N)*N)*", #Default simple noun phrase. See manual.
#pattern = "((A(CA)*|N)*N((P(CP)*)+(D(CD)*)*(A(CA)*|N)*N)*(C(D(CD)*)*(A(CA)*|N)*N((P(CP)*)+(D(CD)*)*(A(CA)*|N)*N)*)*)", #From manual. Noun phrase with coordination conjuction. Este patrón es capaz de extraer ligeramente mas cantidad de terminos, pero tambien tarda una cantidad de tiempo mayor.
#See also this package to know who/why and how is a full noun phrase.
#MÃ¡s abajo estÃ¡n los ejemplos del manual pdf de esta funciÃ³n.
#AquÃ� se explica el pattern: (1) http://brenocon.com/oconnor_textasdata2016.pdf (slides informales)
#                            (2) https://www.aclweb.org/anthology/W16-5615.pdf  (paper duro y fullNP grammar en el apÃ©ndice del paper)
is_regex = TRUE,
#ngram_max. Por defecto es 8. It is an integer indicating to allow phrases to be found up to ngram maximum number of terms following each other. Only used if is_regex is set to TRUE.
#sep = " ". Character indicating how to collapse the phrase of terms which are found. Defaults to using a space.
detailed = TRUE #logical indicating to return the exact positions where the phrase was found (set to TRUE) or just how many times each phrase is occurring (set to FALSE). Defaults to TRUE.
#With TRUE you get keyword, ngran, pattern, start, end
#With FALSE you get keyword, ngram, freq, key
)
x$phrase_tag <- as_phrasemachine(x$upos,
type = "upos" #Puede ser tambiÃ©n "penn-treebank"
)#Convierte los tags de upos a phrasemachine (Handler 2016).
stats3  <- keywords_phrases(x = x$phrase_tag,
term = tolower(x$token),
#pattern = "(A|N)*N(P+D*(A|N)*N)*", #Default simple noun phrase. See manual.
pattern = "((A(CA)*|N)*N((P(CP)*)+(D(CD)*)*(A(CA)*|N)*N)*(C(D(CD)*)*(A(CA)*|N)*N((P(CP)*)+(D(CD)*)*(A(CA)*|N)*N)*)*)", #From manual. Noun phrase with coordination conjuction
#See also this package to know who/why and how is a full noun phrase.
#MÃ¡s abajo estÃ¡n los ejemplos del manual pdf de esta funciÃ³n.
#AquÃ� se explica el pattern: (1) http://brenocon.com/oconnor_textasdata2016.pdf (slides informales)
#                            (2) https://www.aclweb.org/anthology/W16-5615.pdf  (paper duro y fullNP grammar en el apÃ©ndice del paper)
is_regex = TRUE,
#ngram_max. Por defecto es 8. It is an integer indicating to allow phrases to be found up to ngram maximum number of terms following each other. Only used if is_regex is set to TRUE.
#sep = " ". Character indicating how to collapse the phrase of terms which are found. Defaults to using a space.
detailed = TRUE #logical indicating to return the exact positions where the phrase was found (set to TRUE) or just how many times each phrase is occurring (set to FALSE). Defaults to TRUE.
#With TRUE you get keyword, ngran, pattern, start, end
#With FALSE you get keyword, ngram, freq, key
)

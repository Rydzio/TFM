---
title: "Extraccion de terminos TFM"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.


Leemos los documentos.
```{r echo=FALSE}
library(readtext)
library(tictoc)

hilos = 8

#font_import()

#Ruta de trabajo
setwd("~/TFM")

#ruta = "/doc"

ruta = "/legal"

#ruta = "/Users/pedrohv/TFM/TerminologíasInteractivas/data/covid19/raw/documents"

#Leer un corpus
tic()
docs <- readtext(paste0(getwd(),ruta, "*"), #Leo todo lo que tenga ese path
                 #docvarsfrom = "filenames", 
                 #docvarnames = c("document", "language"),
                 #dvsep = "_", 
                 encoding = "UTF-8-BOM", #"ISO-8859-1", #Casi mejor no pongo nada porque no sÃ© el encoding
                 verbosity = 3) 

toc()

print("Se han leido los documentos del corpus con éxito")

```


Creamos el corpus de quanteda
```{r}
library(quanteda)
tic()
# create quanteda corpus
quanteda_options(threads = hilos)
quancorpusDocs <- corpus(docs)

#Obtenemos un resumen del corpus que hemops creado
summ <- summary(quancorpusDocs,    #Esto tarda unos segundos. Types es el num de tokens Únicos.
                n = nrow(docs))    #Por defecto son 100
sum(summ$Sentences)
sum(summ$Tokens)

#Puedo sacar los textos 
tDocs <- texts(quancorpusDocs) #No tarda nada. 
                       #Un vector nombrado (cada elemento tiene el nombre del doc). 
                       #Cada elemento es una cadena con el texto del doc.
toc()
```


Descargamos el modelo udpipe de Google y extraemos los terminos.
```{r}
library(udpipe)

model <- udpipe_download_model(language = "spanish")

path <- model$file_model

tic()
x <- udpipe(tDocs, path, parallel.cores = hilos)
toc()
```


Extraemos las keyword a traves de POS pattern
```{r}
x$phrase_tag <- as_phrasemachine(x$upos, 
                                 type = "upos" #Puede ser tambiÃ©n "penn-treebank"
                                )#Convierte los tags de upos a phrasemachine (Handler 2016). 
tic()
statsPOS <- keywords_phrases(x = x$phrase_tag, 
                          term = tolower(x$token), 
                          pattern = "N(A|N)*N(PD*(A|N)*)*",
                          is_regex = TRUE, 
                          detailed = FALSE 
)
toc()
statsPOS
```


Extracción de keywords a traves de POS pattern pero realizada en cada uno de los documentos del corpus de manera individual
```{r}
library(dplyr)

x$phrase_tag <- as_phrasemachine(x$upos, 
                                 type = "upos" #Puede ser tambiÃ©n "penn-treebank"
                                )#Convierte los tags de upos a phrasemachine (Handler 2016). 
tic()
statsPOS2 <- data.frame()
statsPOSSplit <- data.frame()
split(x, x$doc_id) -> xSplit


for (doc_id in xSplit) {

  statstTemp <- keywords_phrases(x = doc_id$phrase_tag, 
                          term = tolower(doc_id$lemma), 
                          pattern = "N(A|N)*N(PD*(A|N)*)*",
                          is_regex = TRUE, 
                          detailed = FALSE 
                        
  )

  cbind(rep(doc_id$doc_id[1], nrow(statstTemp)), statstTemp) ->> statsPOS2
  
  colnames(statsPOS2)[1] <- "doc_id"
  
  rbind(statsPOS2, statsPOSSplit) ->> statsPOSSplit
  
}
toc()
statsPOSSplit

```



```{r}
total_words <- statsPOSSplit %>% 
  group_by(doc_id) %>% 
  summarize(total = sum(freq))

statsPOSSplit <- left_join(statsPOSSplit, total_words)

statsPOSSplit
```


```{r, fig.width = 20, fig.height = 10}
library(ggplot2)

ggplot(statsPOSSplit, aes(freq/total, fill = doc_id)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~doc_id, ncol = 4, scales = "free_y")
```


```{r}
freq_by_rank <- statsPOSSplit %>% 
  group_by(doc_id) %>% 
  mutate(rank = row_number(), 
         `term frequency` = freq/total)

freq_by_rank
```


```{r}
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = doc_id)) + 
  geom_abline(intercept = -0.62, slope = -1.1, color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()
```

TF_IDF
```{r}
library(tidytext)

statsPOSSplit <- statsPOSSplit %>%
  bind_tf_idf(keyword, doc_id, freq)

statsPOSSplit
```

Presentamos los terminos con mayors puntuación tf_idf en función del documento en el que aparecen en una serie de histogramas
```{r, fig.width = 20, fig.height = 40}
statsPOSSplit %>%
  arrange(desc(tf_idf)) %>%
  mutate(keyword = factor(keyword, levels = rev(unique(keyword)))) %>% 
  group_by(doc_id) %>% 
  top_n(15) %>% 
  ungroup() %>%
  ggplot(aes(keyword, tf_idf, fill = doc_id)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~doc_id, ncol = 2, scales = "free") +
  coord_flip()
```

RAKE Score para las palabras divididas en funhción del documento al que pertenecen
```{r}
probando <- strsplit(statsPOSSplit$keyword, split = " ")
data.frame(splited = unlist(probando)) -> probando

table(probando) -> tabla
tabla <- data.frame(tabla)
```

Puntuación RAKE dividida por documentos
```{r}
score = c()

for (keyIndex in 1:nrow(statsPOSSplit)) {
  
  if(statsPOSSplit[keyIndex, "ngram"] == 1){
    score <- append(score, 0)
  } else {

    keySplt <- strsplit(statsPOSSplit[keyIndex, "keyword"], " ")
    keyDegree <<- statsPOSSplit[keyIndex, "ngram"] - 1
  
    tempScore <- c()
    for(word in keySplt){
      
      tempScore <- append(tempScore, keyDegree / tabla[tabla$probando %in% word, ]$Freq)
      
    }
    score <- append(score, sum(tempScore))
  }
}


```


Presentamos las palabras con mayor puntuación RAKE divididas en función de los documentos en las que aparecen en una tabla
```{r}

cbind(statsPOSSplit, score) -> statsPOSRAKE
colnames(statsPOSRAKE)[9] <- "RAKE"
statsPOSRAKE <- statsPOSRAKE[order(-statsPOSRAKE$RAKE),]
statsPOSRAKE
```


Presentamos las palabras con mayor puntuación RAKE divididas en función de los documentos en las que aparecen en una serie de histogramas
```{r, fig.width = 20, fig.height = 40}
library(ggplot2)

statsPOSRAKE %>%
  arrange(desc(RAKE)) %>%
  mutate(keyword = factor(keyword, levels = rev(unique(keyword)))) %>% 
  group_by(doc_id) %>% 
  top_n(15) %>% 
  ungroup() %>%
  ggplot(aes(keyword, RAKE, fill = doc_id)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "RAKE") +
  facet_wrap(~doc_id, ncol = 2, scales = "free") +
  coord_flip()
```


RAKE score para el conjunto del corpus completo.
```{r}
probandoPOS <- strsplit(statsPOS$keyword, split = " ")
data.frame(splited = unlist(probandoPOS)) -> probandoPOS

table(probandoPOS) -> tablaPOS
tablaPOS <- data.frame(tablaPOS)
```


Extraccion de RAKE para todos los terminos del corpus completo
```{r}
score = c()

for (keyIndex in 1:nrow(statsPOS)) {
  
  if(statsPOS[keyIndex, "ngram"] == 1){
    score <- append(score, 0)
  } else {

    keySplt <- strsplit(statsPOS[keyIndex, "keyword"], " ")
    keyDegree <<- statsPOS[keyIndex, "ngram"] - 1
  
    tempScore <- c()
    for(word in keySplt){
      
      tempScore <- append(tempScore, keyDegree / tablaPOS[tablaPOS$probandoPOS %in% word, ]$Freq)
      
    }
    score <- append(score, sum(tempScore))
  }
}


```


Unimos el resultado de la puntuación RAKE y presentamos la tabla
```{r}

cbind(statsPOS, score) -> statsPOSRAKEScore
colnames(statsPOSRAKEScore)[4] <- "RAKE"
statsPOSRAKEScore <- statsPOSRAKEScore[order(-statsPOSRAKEScore$RAKE),]
statsPOSRAKEScore
```

Presentamos las keywords de todo el corpus puntuadas a través de RAKE en un histograma
```{r, fig.width = 20, fig.height = 10}
library(forcats)

statsPOSRAKEScore %>%
  mutate(keyword = fct_reorder(keyword, RAKE)) %>%
  top_n(50) %>% 
  ggplot() + geom_col(aes(x = keyword, y = RAKE), position = "dodge") + 
  coord_flip()
```


RAKE normal. La extraccion a través de RAKE conjuntamente con la puntuación RAKE.
```{r}
stats <- keywords_rake(x = x, 
                       term = "lemma", 
                       group = "doc_id", 
                       relevant = x$upos %in% c("NOUN", "ADJ"))
stats
```


C-value para el conjunto del corpus completo.

**OJO
Aquí hay un problema grave con el tiempo de carga. El algoritmo para calcular c-value requiere obtener todos aquellos terminos en los que se contenga una palabra, esto es: Para calcular el c-value de "se", se requiere obtener todos y cada uno de los terminos que contengan la palabra "se", como por ejemplo ""


```{r}
statsPOS <- statsPOS[order(statsPOS$freq),]

cValue <- c()
tic()
for (keyIndex in 1:nrow(statsPOS)) {
  #Extraemos el termino candidato
  candidate <- statsPOS[keyIndex, "keyword"]
  #Extraemos la frecuencia del candidato en el corpùs
  freqCandidate <- statsPOS[keyIndex, "freq"]

  #Buscamos terminos que contengan a nuestro candidato. Tarda 0.02 segundos de media por iteracción en un i9-9880H en un termino normal. Con un termino MUY frecuente,      puede tardar minutos! No parece haber ninguna forma de calcular mas eficientemente esta característica.
  coincidencias <- statsPOS[grepl(candidate ,statsPOS$keyword, fixed = TRUE), ]

  #Numero de coincidencias
  ncoincidencias <- nrow(coincidencias)
  if(ncoincidencias == 1){
    #El candidato no esta contenido en otro termino
    #Calculamos c-value
    res <- log2(length(strsplit(candidate, " ")[[1]])) * freqCandidate
    #Almacenamos el resultado
    append(cValue, res) -> cValue 
  } else {
    sumatorio <- sum(coincidencias$freq)
    res <- log2(length(strsplit(candidate, " ")[[1]])) * freqCandidate - (1 / ncoincidencias) * sumatorio
    #Almacenamos el resultado
    append(cValue, res) -> cValue 
  }
}
toc()
print("fin")

cbind(statsPOS, cValue) -> statsPOSCVALUE
colnames(statsPOSCVALUE)[4] <- "cvalue"

```

Ordenamos la tabla y la presentamos
```{r}

statsPOSCVALUE <- statsPOSCVALUE[order(-statsPOSCVALUE$cvalue),]

statsPOSCVALUE

```

Presentamos las keyword ordenadas por c-value en una grafica de histogramas
```{r, fig.width = 20, fig.height = 20}
library(forcats)

statsPOSCVALUE %>%
  mutate(keyword = fct_reorder(keyword, cvalue)) %>%
  top_n(20) %>% 
  ggplot() + geom_col(aes(x = keyword, y = cvalue), position = "dodge") + 
  coord_flip()
```































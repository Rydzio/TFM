tempScore <- c()
for(word in keySplt){
tempScore <- append(tempScore, keyDegree / tabla[tabla$probando %in% word, ]$Freq)
}
score <- append(score, sum(tempScore))
}
}
cbind(statsPOSSplit, score) -> statsPOSRAKE
colnames(statsPOSRAKE)[9] <- "RAKE"
statsPOSRAKE <- statsPOSRAKE[order(-statsPOSRAKE$RAKE),]
statsPOSRAKE
library(ggplot2)
statsPOSRAKE %>%
arrange(desc(RAKE)) %>%
mutate(keyword = factor(keyword, levels = rev(unique(keyword)))) %>%
group_by(doc_id) %>%
top_n(15) %>%
ungroup() %>%
ggplot(aes(keyword, RAKE, fill = doc_id)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "RAKE") +
facet_wrap(~doc_id, ncol = 2, scales = "free") +
coord_flip()
probandoPOS <- strsplit(statsPOS$keyword, split = " ")
data.frame(splited = unlist(probandoPOS)) -> probandoPOS
table(probandoPOS) -> tablaPOS
tablaPOS <- data.frame(tablaPOS)
score = c()
for (keyIndex in 1:nrow(statsPOS)) {
if(statsPOS[keyIndex, "ngram"] == 1){
score <- append(score, 0)
} else {
keySplt <- strsplit(statsPOS[keyIndex, "keyword"], " ")
keyDegree <<- statsPOS[keyIndex, "ngram"] - 1
tempScore <- c()
for(word in keySplt){
tempScore <- append(tempScore, keyDegree / tablaPOS[tablaPOS$probandoPOS %in% word, ]$Freq)
}
score <- append(score, sum(tempScore))
}
}
cbind(statsPOS, score) -> statsPOSRAKEScore
colnames(statsPOSRAKEScore)[4] <- "RAKE"
statsPOSRAKEScore <- statsPOSRAKEScore[order(-statsPOSRAKEScore$RAKE),]
statsPOSRAKEScore
library(forcats)
statsPOSRAKEScore %>%
mutate(keyword = fct_reorder(keyword, RAKE)) %>%
top_n(50) %>%
ggplot() + geom_col(aes(x = keyword, y = RAKE), position = "dodge") +
coord_flip()
stats <- keywords_rake(x = x,
term = "lemma",
group = "doc_id",
relevant = x$upos %in% c("NOUN", "ADJ"))
stats
statsPOS <- statsPOS[order(statsPOS$freq),]
cValue <- c()
tic()
for (keyIndex in 1:nrow(statsPOS)) {
tic()
#Extraemos el termino candidato
candidate <- statsPOS[keyIndex, "keyword"]
#Extraemos la frecuencia del candidato en el corpùs
freqCandidate <- statsPOS[keyIndex, "freq"]
print(candidate)
#Buscamos terminos que contengan a nuestro candidato. Tarda 0.02 segundos de media por iteracción en un i9-9880H en un termino normal. Con un termino MUY frecuente,      puede tardar minutos! No parece haber ninguna forma de calcular mas eficientemente esta característica.
coincidencias <- statsPOS[grepl(candidate ,statsPOS$keyword, fixed = TRUE), ]$keyword
#Numero de coincidencias
ncoincidencias <- length(coincidencias)
if(ncoincidencias == 1){
#El candidato no esta contenido en otro termino
#Calculamos c-value
res <- log2(length(strsplit(candidate, " ")[[1]])) * freqCandidate
#Almacenamos el resultado
append(cValue, res) -> cValue
} else {
#El candidato está contenido en al menos un termino más
sumatorio = 0
for (word in coincidencias) {
if(!(statsPOS[statsPOS$keyword %in% word, ]$keyword %in% candidate)) {
sumatorio = sumatorio + statsPOS[statsPOS$keyword == word, ]$freq
}
}
res <- log2(length(strsplit(candidate, " ")[[1]])) * freqCandidate - (1 / ncoincidencias) * sumatorio
#Almacenamos el resultado
append(cValue, res) -> cValue
}
toc()
}
statsPOS <- statsPOS[order(statsPOS$freq),]
cValue <- c()
tic()
for (keyIndex in 1:nrow(statsPOS)) {
#Extraemos el termino candidato
candidate <- statsPOS[keyIndex, "keyword"]
#Extraemos la frecuencia del candidato en el corpùs
freqCandidate <- statsPOS[keyIndex, "freq"]
#Buscamos terminos que contengan a nuestro candidato. Tarda 0.02 segundos de media por iteracción en un i9-9880H en un termino normal. Con un termino MUY frecuente,      puede tardar minutos! No parece haber ninguna forma de calcular mas eficientemente esta característica.
coincidencias <- statsPOS[grepl(candidate ,statsPOS$keyword, fixed = TRUE), ]$keyword
#Numero de coincidencias
ncoincidencias <- length(coincidencias)
if(ncoincidencias == 1){
#El candidato no esta contenido en otro termino
#Calculamos c-value
res <- log2(length(strsplit(candidate, " ")[[1]])) * freqCandidate
#Almacenamos el resultado
append(cValue, res) -> cValue
} else {
#El candidato está contenido en al menos un termino más
sumatorio = 0
for (word in coincidencias) {
if(!(statsPOS[statsPOS$keyword %in% word, ]$keyword %in% candidate)) {
sumatorio = sumatorio + statsPOS[statsPOS$keyword == word, ]$freq
}
}
res <- log2(length(strsplit(candidate, " ")[[1]])) * freqCandidate - (1 / ncoincidencias) * sumatorio
#Almacenamos el resultado
append(cValue, res) -> cValue
}
}
toc()
print("fin")
cbind(statsPOS, cValue) -> statsPOSCVALUE
colnames(statsPOSCVALUE)[4] <- "cvalue"
statsPOSCVALUE <- statsPOSCVALUE[order(-statsPOSCVALUE$cvalue),]
statsPOSCVALUE
library(forcats)
statsPOSCVALUE %>%
mutate(keyword = fct_reorder(keyword, cvalue)) %>%
top_n(50) %>%
ggplot() + geom_col(aes(x = keyword, y = cvalue), position = "dodge") +
coord_flip()
library(forcats)
statsPOSCVALUE %>%
mutate(keyword = fct_reorder(keyword, cvalue)) %>%
top_n(20) %>%
ggplot() + geom_col(aes(x = keyword, y = cvalue), position = "dodge") +
coord_flip()
library(forcats)
statsPOSCVALUE %>%
mutate(keyword = fct_reorder(keyword, cvalue)) %>%
top_n(20) %>%
ggplot() + geom_col(aes(x = keyword, y = cvalue), position = "dodge") +
coord_flip()
statsPOS <- statsPOS[order(statsPOS$freq),]
cValue <- c()
tic()
for (keyIndex in 1:nrow(statsPOS)) {
#Extraemos el termino candidato
candidate <- statsPOS[keyIndex, "keyword"]
#Extraemos la frecuencia del candidato en el corpùs
freqCandidate <- statsPOS[keyIndex, "freq"]
#Buscamos terminos que contengan a nuestro candidato. Tarda 0.02 segundos de media por iteracción en un i9-9880H en un termino normal. Con un termino MUY frecuente,      puede tardar minutos! No parece haber ninguna forma de calcular mas eficientemente esta característica.
coincidencias <- statsPOS[grepl(candidate ,statsPOS$keyword, fixed = TRUE), ]
#Numero de coincidencias
# ncoincidencias <- length(coincidencias)
ncoincidencias <- nrow(coincidencias)
if(ncoincidencias == 1){
#El candidato no esta contenido en otro termino
#Calculamos c-value
res <- log2(length(strsplit(candidate, " ")[[1]])) * freqCandidate
#Almacenamos el resultado
append(cValue, res) -> cValue
} else {
#El candidato está contenido en al menos un termino más
# sumatorio = 0
# for (word in coincidencias) {
#   if(!(statsPOS[statsPOS$keyword %in% word, ]$keyword %in% candidate)) {
#     sumatorio = sumatorio + statsPOS[statsPOS$keyword == word, ]$freq
#   }
# }
sumatorio <- sum(coincidencias$freq)
res <- log2(length(strsplit(candidate, " ")[[1]])) * freqCandidate - (1 / ncoincidencias) * sumatorio
#Almacenamos el resultado
append(cValue, res) -> cValue
}
}
toc()
print("fin")
cbind(statsPOS, cValue) -> statsPOSCVALUE
colnames(statsPOSCVALUE)[4] <- "cvalue"
statsPOSCVALUE <- statsPOSCVALUE[order(-statsPOSCVALUE$cvalue),]
statsPOSCVALUE
library(forcats)
statsPOSCVALUE %>%
mutate(keyword = fct_reorder(keyword, cvalue)) %>%
top_n(20) %>%
ggplot() + geom_col(aes(x = keyword, y = cvalue), position = "dodge") +
coord_flip()
statsPOS <- statsPOS[order(statsPOS$freq),]
cValue <- c()
tic()
for (keyIndex in 1:nrow(statsPOS)) {
#Extraemos el termino candidato
candidate <- statsPOS[keyIndex, "keyword"]
#Extraemos la frecuencia del candidato en el corpùs
freqCandidate <- statsPOS[keyIndex, "freq"]
#Buscamos terminos que contengan a nuestro candidato. Tarda 0.02 segundos de media por iteracción en un i9-9880H en un termino normal. Con un termino MUY frecuente,      puede tardar minutos! No parece haber ninguna forma de calcular mas eficientemente esta característica.
coincidencias <- statsPOS[grepl(candidate ,statsPOS$keyword, fixed = TRUE), ]
#Numero de coincidencias
ncoincidencias <- nrow(coincidencias)
if(ncoincidencias == 1){
#El candidato no esta contenido en otro termino
#Calculamos c-value
res <- log2(length(strsplit(candidate, " ")[[1]])) * freqCandidate
#Almacenamos el resultado
append(cValue, res) -> cValue
} else {
sumatorio <- sum(coincidencias$freq)
res <- log2(length(strsplit(candidate, " ")[[1]])) * freqCandidate - (1 / ncoincidencias) * sumatorio
#Almacenamos el resultado
append(cValue, res) -> cValue
}
}
toc()
print("fin")
cbind(statsPOS, cValue) -> statsPOSCVALUE
colnames(statsPOSCVALUE)[4] <- "cvalue"
statsPOSCVALUE <- statsPOSCVALUE[order(-statsPOSCVALUE$cvalue),]
statsPOSCVALUE
library(readtext)
library(tictoc)
hilos = 8
#font_import()
#Ruta de trabajo
setwd("~/TFM")
#ruta = "/doc"
ruta = "/legal"
#ruta = "/Users/pedrohv/TFM/TerminologíasInteractivas/data/covid19/raw/documents"
#Leer un corpus
tic()
docs <- readtext(paste0(getwd(),ruta, "*"), #Leo todo lo que tenga ese path
#docvarsfrom = "filenames",
#docvarnames = c("document", "language"),
#dvsep = "_",
encoding = "UTF-8-BOM", #"ISO-8859-1", #Casi mejor no pongo nada porque no sÃ© el encoding
verbosity = 3)
toc()
print("Se han leido los documentos del corpus con éxito")
library(quanteda)
tic()
# create quanteda corpus
quanteda_options(threads = hilos)
quancorpusDocs <- corpus(docs)
#Obtenemos un resumen del corpus que hemops creado
summ <- summary(quancorpusDocs,    #Esto tarda unos segundos. Types es el num de tokens Únicos.
n = nrow(docs))    #Por defecto son 100
sum(summ$Sentences)
sum(summ$Tokens)
#Puedo sacar los textos
tDocs <- texts(quancorpusDocs) #No tarda nada.
#Un vector nombrado (cada elemento tiene el nombre del doc).
#Cada elemento es una cadena con el texto del doc.
toc()
library(udpipe)
model <- udpipe_download_model(language = "spanish")
path <- model$file_model
tic()
x <- udpipe(tDocs, path, parallel.cores = hilos)
library(readtext)
library(tictoc)
hilos = 8
#font_import()
#Ruta de trabajo
setwd("~/TFM")
#ruta = "/doc"
ruta = "/legal"
#ruta = "/Users/pedrohv/TFM/TerminologíasInteractivas/data/covid19/raw/documents"
#Leer un corpus
tic()
docs <- readtext(paste0(getwd(),ruta, "*"), #Leo todo lo que tenga ese path
#docvarsfrom = "filenames",
#docvarnames = c("document", "language"),
#dvsep = "_",
encoding = "UTF-8-BOM", #"ISO-8859-1", #Casi mejor no pongo nada porque no sÃ© el encoding
verbosity = 3)
toc()
print("Se han leido los documentos del corpus con éxito")
library(quanteda)
tic()
# create quanteda corpus
quanteda_options(threads = hilos)
quancorpusDocs <- corpus(docs)
#Obtenemos un resumen del corpus que hemops creado
summ <- summary(quancorpusDocs,    #Esto tarda unos segundos. Types es el num de tokens Únicos.
n = nrow(docs))    #Por defecto son 100
sum(summ$Sentences)
sum(summ$Tokens)
#Puedo sacar los textos
tDocs <- texts(quancorpusDocs) #No tarda nada.
#Un vector nombrado (cada elemento tiene el nombre del doc).
#Cada elemento es una cadena con el texto del doc.
toc()
library(udpipe)
model <- udpipe_download_model(language = "spanish")
path <- model$file_model
tic()
x <- udpipe(tDocs, path, parallel.cores = hilos)
library(readtext)
library(tictoc)
hilos = 8
#font_import()
#Ruta de trabajo
setwd("~/TFM")
#ruta = "/doc"
ruta = "/legal"
#ruta = "/Users/pedrohv/TFM/TerminologíasInteractivas/data/covid19/raw/documents"
#Leer un corpus
tic()
docs <- readtext(paste0(getwd(),ruta, "*"), #Leo todo lo que tenga ese path
#docvarsfrom = "filenames",
#docvarnames = c("document", "language"),
#dvsep = "_",
encoding = "UTF-8-BOM", #"ISO-8859-1", #Casi mejor no pongo nada porque no sÃ© el encoding
verbosity = 3)
toc()
print("Se han leido los documentos del corpus con éxito")
library(quanteda)
tic()
# create quanteda corpus
quanteda_options(threads = hilos)
quancorpusDocs <- corpus(docs)
#Obtenemos un resumen del corpus que hemops creado
summ <- summary(quancorpusDocs,    #Esto tarda unos segundos. Types es el num de tokens Únicos.
n = nrow(docs))    #Por defecto son 100
sum(summ$Sentences)
sum(summ$Tokens)
#Puedo sacar los textos
tDocs <- texts(quancorpusDocs) #No tarda nada.
#Un vector nombrado (cada elemento tiene el nombre del doc).
#Cada elemento es una cadena con el texto del doc.
toc()
library(udpipe)
model <- udpipe_download_model(language = "spanish")
path <- model$file_model
tic()
x <- udpipe(tDocs, path, parallel.cores = hilos)
toc()
x$phrase_tag <- as_phrasemachine(x$upos,
type = "upos" #Puede ser tambiÃ©n "penn-treebank"
)#Convierte los tags de upos a phrasemachine (Handler 2016).
tic()
statsPOS <- keywords_phrases(x = x$phrase_tag,
term = tolower(x$token),
pattern = "N(A|N)*N(PD*(A|N)*)*",
is_regex = TRUE,
detailed = FALSE
)
toc()
statsPOS
library(dplyr)
x$phrase_tag <- as_phrasemachine(x$upos,
type = "upos" #Puede ser tambiÃ©n "penn-treebank"
)#Convierte los tags de upos a phrasemachine (Handler 2016).
tic()
statsPOS2 <- data.frame()
statsPOSSplit <- data.frame()
split(x, x$doc_id) -> xSplit
for (doc_id in xSplit) {
statstTemp <- keywords_phrases(x = doc_id$phrase_tag,
term = tolower(doc_id$lemma),
pattern = "N(A|N)*N(PD*(A|N)*)*",
is_regex = TRUE,
detailed = FALSE
)
cbind(rep(doc_id$doc_id[1], nrow(statstTemp)), statstTemp) ->> statsPOS2
colnames(statsPOS2)[1] <- "doc_id"
rbind(statsPOS2, statsPOSSplit) ->> statsPOSSplit
}
toc()
statsPOSSplit
total_words <- statsPOSSplit %>%
group_by(doc_id) %>%
summarize(total = sum(freq))
statsPOSSplit <- left_join(statsPOSSplit, total_words)
statsPOSSplit
library(ggplot2)
ggplot(statsPOSSplit, aes(freq/total, fill = doc_id)) +
geom_histogram(show.legend = FALSE) +
xlim(NA, 0.0009) +
facet_wrap(~doc_id, ncol = 4, scales = "free_y")
freq_by_rank <- statsPOSSplit %>%
group_by(doc_id) %>%
mutate(rank = row_number(),
`term frequency` = freq/total)
freq_by_rank
freq_by_rank %>%
ggplot(aes(rank, `term frequency`, color = doc_id)) +
geom_abline(intercept = -0.62, slope = -1.1, color = "gray50", linetype = 2) +
geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
scale_x_log10() +
scale_y_log10()
library(tidytext)
statsPOSSplit <- statsPOSSplit %>%
bind_tf_idf(keyword, doc_id, freq)
statsPOSSplit
statsPOSSplit %>%
arrange(desc(tf_idf)) %>%
mutate(keyword = factor(keyword, levels = rev(unique(keyword)))) %>%
group_by(doc_id) %>%
top_n(15) %>%
ungroup() %>%
ggplot(aes(keyword, tf_idf, fill = doc_id)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~doc_id, ncol = 2, scales = "free") +
coord_flip()
probando <- strsplit(statsPOSSplit$keyword, split = " ")
data.frame(splited = unlist(probando)) -> probando
table(probando) -> tabla
tabla <- data.frame(tabla)
score = c()
for (keyIndex in 1:nrow(statsPOSSplit)) {
if(statsPOSSplit[keyIndex, "ngram"] == 1){
score <- append(score, 0)
} else {
keySplt <- strsplit(statsPOSSplit[keyIndex, "keyword"], " ")
keyDegree <<- statsPOSSplit[keyIndex, "ngram"] - 1
tempScore <- c()
for(word in keySplt){
tempScore <- append(tempScore, keyDegree / tabla[tabla$probando %in% word, ]$Freq)
}
score <- append(score, sum(tempScore))
}
}
cbind(statsPOSSplit, score) -> statsPOSRAKE
colnames(statsPOSRAKE)[9] <- "RAKE"
statsPOSRAKE <- statsPOSRAKE[order(-statsPOSRAKE$RAKE),]
statsPOSRAKE
library(ggplot2)
statsPOSRAKE %>%
arrange(desc(RAKE)) %>%
mutate(keyword = factor(keyword, levels = rev(unique(keyword)))) %>%
group_by(doc_id) %>%
top_n(15) %>%
ungroup() %>%
ggplot(aes(keyword, RAKE, fill = doc_id)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "RAKE") +
facet_wrap(~doc_id, ncol = 2, scales = "free") +
coord_flip()
probandoPOS <- strsplit(statsPOS$keyword, split = " ")
data.frame(splited = unlist(probandoPOS)) -> probandoPOS
table(probandoPOS) -> tablaPOS
tablaPOS <- data.frame(tablaPOS)
score = c()
for (keyIndex in 1:nrow(statsPOS)) {
if(statsPOS[keyIndex, "ngram"] == 1){
score <- append(score, 0)
} else {
keySplt <- strsplit(statsPOS[keyIndex, "keyword"], " ")
keyDegree <<- statsPOS[keyIndex, "ngram"] - 1
tempScore <- c()
for(word in keySplt){
tempScore <- append(tempScore, keyDegree / tablaPOS[tablaPOS$probandoPOS %in% word, ]$Freq)
}
score <- append(score, sum(tempScore))
}
}
cbind(statsPOS, score) -> statsPOSRAKEScore
colnames(statsPOSRAKEScore)[4] <- "RAKE"
statsPOSRAKEScore <- statsPOSRAKEScore[order(-statsPOSRAKEScore$RAKE),]
statsPOSRAKEScore
library(forcats)
statsPOSRAKEScore %>%
mutate(keyword = fct_reorder(keyword, RAKE)) %>%
top_n(50) %>%
ggplot() + geom_col(aes(x = keyword, y = RAKE), position = "dodge") +
coord_flip()
stats <- keywords_rake(x = x,
term = "lemma",
group = "doc_id",
relevant = x$upos %in% c("NOUN", "ADJ"))
stats
statsPOS <- statsPOS[order(statsPOS$freq),]
cValue <- c()
tic()
for (keyIndex in 1:nrow(statsPOS)) {
#Extraemos el termino candidato
candidate <- statsPOS[keyIndex, "keyword"]
#Extraemos la frecuencia del candidato en el corpùs
freqCandidate <- statsPOS[keyIndex, "freq"]
#Buscamos terminos que contengan a nuestro candidato. Tarda 0.02 segundos de media por iteracción en un i9-9880H en un termino normal. Con un termino MUY frecuente,      puede tardar minutos! No parece haber ninguna forma de calcular mas eficientemente esta característica.
coincidencias <- statsPOS[grepl(candidate ,statsPOS$keyword, fixed = TRUE), ]
#Numero de coincidencias
ncoincidencias <- nrow(coincidencias)
if(ncoincidencias == 1){
#El candidato no esta contenido en otro termino
#Calculamos c-value
res <- log2(length(strsplit(candidate, " ")[[1]])) * freqCandidate
#Almacenamos el resultado
append(cValue, res) -> cValue
} else {
sumatorio <- sum(coincidencias$freq)
res <- log2(length(strsplit(candidate, " ")[[1]])) * freqCandidate - (1 / ncoincidencias) * sumatorio
#Almacenamos el resultado
append(cValue, res) -> cValue
}
}
toc()
print("fin")
cbind(statsPOS, cValue) -> statsPOSCVALUE
colnames(statsPOSCVALUE)[4] <- "cvalue"
statsPOSCVALUE <- statsPOSCVALUE[order(-statsPOSCVALUE$cvalue),]
statsPOSCVALUE
library(forcats)
statsPOSCVALUE %>%
mutate(keyword = fct_reorder(keyword, cvalue)) %>%
top_n(20) %>%
ggplot() + geom_col(aes(x = keyword, y = cvalue), position = "dodge") +
coord_flip()

eyDegree / tabla[tabla$probando %in% x, ]$Freq
}
fprueba = function(x){
if(as.integer(x[3]) == 1){
0
} else {
keySplt <- strsplit(x[2], " ")[[1]]
keyDegree <<- as.integer(x[3]) - 1
tempScore <- c()
# print(keySplt)
# print(class(keySplt))
tempScore <- lapply(keySplt, fpruebaInterno)
print(tempScore)
sum(tempScore)
}
}
score3 <- apply(statsPOSSplit, 1, fprueba)
tic()
score3 = c()
fpruebaInterno = function(x) {
print(x)
keyDegree / tabla[tabla$probando %in% x, ]$Freq
}
fprueba = function(x){
if(as.integer(x[3]) == 1){
0
} else {
keySplt <- strsplit(x[2], " ")[[1]]
keyDegree <<- as.integer(x[3]) - 1
tempScore <- c()
# print(keySplt)
# print(class(keySplt))
tempScore <- lapply(keySplt, fpruebaInterno)
print(tempScore)
sum(tempScore)
}
}
score3 <- apply(statsPOSSplit, 1, fprueba)
tic()
score3 = c()
fpruebaInterno = function(x) {
print(x)
keyDegree / tabla[tabla$probando %in% x, ]$Freq
}
fprueba = function(x){
if(as.integer(x[3]) == 1){
0
} else {
keySplt <- strsplit(x[2], " ")[[1]]
keyDegree <<- as.integer(x[3]) - 1
tempScore <- c()
print(keySplt)
# print(class(keySplt))
tempScore <- lapply(keySplt, fpruebaInterno)
print(tempScore)
sum(tempScore)
}
}
score3 <- apply(statsPOSSplit, 1, fprueba)
tic()
score3 = c()
fpruebaInterno = function(x) {
print(x)
keyDegree / tabla[tabla$probando %in% x, ]$Freq
}
fprueba = function(x){
if(as.integer(x[3]) == 1){
0
} else {
keySplt <- strsplit(x[2], " ")[[1]]
keyDegree <<- as.integer(x[3]) - 1
tempScore <- c()
print(keySplt)
# print(class(keySplt))
tempScore <- lapply(keySplt, fpruebaInterno)
print(class(tempScore))
sum(tempScore)
}
}
score3 <- apply(statsPOSSplit, 1, fprueba)
tic()
score3 = c()
fpruebaInterno = function(x) {
print(x)
keyDegree / tabla[tabla$probando %in% x, ]$Freq
}
fprueba = function(x){
if(as.integer(x[3]) == 1){
0
} else {
keySplt <- strsplit(x[2], " ")[[1]]
keyDegree <<- as.integer(x[3]) - 1
tempScore <- c()
print(keySplt)
# print(class(keySplt))
tempScore <- lapply(keySplt, fpruebaInterno)
print(class(tempScore[[1]]))
sum(tempScore)
}
}
score3 <- apply(statsPOSSplit, 1, fprueba)
tic()
score3 = c()
fpruebaInterno = function(x) {
print(x)
keyDegree / tabla[tabla$probando %in% x, ]$Freq
}
fprueba = function(x){
if(as.integer(x[3]) == 1){
0
} else {
keySplt <- strsplit(x[2], " ")[[1]]
keyDegree <<- as.integer(x[3]) - 1
tempScore <- c()
print(keySplt)
# print(class(keySplt))
tempScore <- lapply(keySplt, fpruebaInterno)
print(class(tempScore[[1]]))
sum(tempScore[[1]])
}
}
score3 <- apply(statsPOSSplit, 1, fprueba)
tic()
score3 = c()
fpruebaInterno = function(x) {
#print(x)
keyDegree / tabla[tabla$probando %in% x, ]$Freq
}
fprueba = function(x){
if(as.integer(x[3]) == 1){
0
} else {
keySplt <- strsplit(x[2], " ")[[1]]
keyDegree <<- as.integer(x[3]) - 1
tempScore <- c()
#print(keySplt)
# print(class(keySplt))
tempScore <- lapply(keySplt, fpruebaInterno)
#print(class(tempScore[[1]]))
sum(tempScore[[1]])
}
}
score3 <- apply(statsPOSSplit, 1, fprueba)
tic()
score3 = c()
fpruebaInterno = function(x) {
#print(x)
keyDegree / tabla[tabla$probando %in% x, ]$Freq
}
fprueba = function(x){
if(as.integer(x[3]) == 1){
0
} else {
keySplt <- strsplit(x[2], " ")[[1]]
keyDegree <<- as.integer(x[3]) - 1
tempScore <- c()
#print(keySplt)
# print(class(keySplt))
tempScore <- lapply(keySplt, fpruebaInterno)
print(class(tempScore[[1]]))
sum(tempScore[[1]])
}
}
score3 <- apply(statsPOSSplit, 1, fprueba)
tic()
score3 = c()
fpruebaInterno = function(x) {
#print(x)
keyDegree / tabla[tabla$probando %in% x, ]$Freq
}
fprueba = function(x){
if(as.integer(x[3]) == 1){
0
} else {
keySplt <- strsplit(x[2], " ")[[1]]
keyDegree <<- as.integer(x[3]) - 1
tempScore <- c()
#print(keySplt)
# print(class(keySplt))
tempScore <- lapply(keySplt, fpruebaInterno)
print(sum(tempScore[[1]]))
sum(tempScore[[1]])
}
}
score3 <- apply(statsPOSSplit, 1, fprueba)
#Agrupamos la terminologia en funcion del documento, generando una terminología completa del corpus entero.
stats <- select(statsPOSRAKEScore, -c(doc_id))
stats <- aggregate(list(Frecuencia=stats$freq, RAKE1=stats$RAKE, RAKE2=stats$RAKE2), by=list(keyword=stats$keyword, ngram=stats$ngram), FUN=sum)
stats
View(stats)
pb = txtProgressBar(min = 0, max = nrow(stats), initial = 0)
cValue <- c()
tic()
for (keyIndex in 1:nrow(stats)) {
#Extraemos el termino candidato
candidate <- stats[keyIndex, "keyword"]
#Extraemos la frecuencia del candidato en el corpùs
freqCandidate <- stats[keyIndex, "freq"]
#Buscamos terminos que contengan a nuestro candidato.
coincidencias <- stats[grepl(candidate ,stats$keyword, fixed = TRUE), ]
#Numero de coincidencias
ncoincidencias <- nrow(coincidencias)
if(ncoincidencias == 1){
#El candidato no esta contenido en otro termino
#Calculamos c-value
res <- log2(length(strsplit(candidate, " ")[[1]])) * freqCandidate
#Almacenamos el resultado
append(cValue, res) -> cValue
} else {
sumatorio <- sum(coincidencias$freq)
res <- log2(length(strsplit(candidate, " ")[[1]])) * freqCandidate - (1 / ncoincidencias) * sumatorio
#Almacenamos el resultado
append(cValue, res) -> cValue
}
setTxtProgressBar(pb,keyIndex)
}
toc()
print("fin")
cbind(stats, cValue) -> statsPOSCVALUE
statsPOSCVALUE
pb = txtProgressBar(min = 0, max = nrow(stats), initial = 0)
cValue <- c()
tic()
library(readtext)
library(doParallel)
library(quanteda)
library(quanteda)
library(stringr)
library(udpipe)
library(dplyr)
pb = txtProgressBar(min = 0, max = nrow(stats), initial = 0)
cValue <- c()
tic()
pb = txtProgressBar(min = 0, max = nrow(stats), initial = 0)
cValue <- c()
tic()
library(tictoc)
pb = txtProgressBar(min = 0, max = nrow(stats), initial = 0)
cValue <- c()
tic()
for (keyIndex in 1:nrow(stats)) {
#Extraemos el termino candidato
candidate <- stats[keyIndex, "keyword"]
#Extraemos la frecuencia del candidato en el corpùs
freqCandidate <- stats[keyIndex, "freq"]
#Buscamos terminos que contengan a nuestro candidato.
coincidencias <- stats[grepl(candidate ,stats$keyword, fixed = TRUE), ]
#Numero de coincidencias
ncoincidencias <- nrow(coincidencias)
if(ncoincidencias == 1){
#El candidato no esta contenido en otro termino
#Calculamos c-value
res <- log2(length(strsplit(candidate, " ")[[1]])) * freqCandidate
#Almacenamos el resultado
append(cValue, res) -> cValue
} else {
sumatorio <- sum(coincidencias$freq)
res <- log2(length(strsplit(candidate, " ")[[1]])) * freqCandidate - (1 / ncoincidencias) * sumatorio
#Almacenamos el resultado
append(cValue, res) -> cValue
break
}
setTxtProgressBar(pb,keyIndex)
}
toc()
print("fin")
cbind(stats, cValue) -> statsPOSCVALUE
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
setwd("~/NLP/Linx/Spanish")
#Leo termcat_collective_negotiation_es.csv
df <- read.csv("termcat_collective_negotiation_es.csv" ,
header=FALSE,
blank.lines.skip = FALSE, #Fundamental!!!! Sin esto no lee líneas vacias
stringsAsFactors = FALSE
)
#Elimino las primeras 9 líneas (cabeceras)
df <- df[-c(1:9),]
#Tiene tres columnas (la última está vacía) RARO!
#Renombro columnas
colnames(df) <- c("uri", "label", "V3")
#which(df$V3 != "")
#Dice que todas las celdas de la tercera columna están vacías.
# create quanteda corpus
library(quanteda)
quanteda_options(threads = 10)
quancorpusDocs <- corpus(df$label)#Cada label es un documento
summ <- summary(quancorpusDocs,    #Esto tarda unos segundos. Types es el num de tokens únicos.
n = length(df$label))#Por defecto son 100
#sum(summ$Sentences) #Salen 719
#sum(summ$Tokens)    #Salen 1718
#Puedo sacar los textos
tDocs <- texts(quancorpusDocs) #No tarda nada.
#Un vector nombrado (cada elemento tiene el nombre del doc).
#Cada elemento es una cadena con el texto del doc.
#Reemplazo en t todas las apariciones de los \00X por ""
library(stringr)
#tmp= c("\001\004AB\006", "AB\005CD\001")
t2  <- str_replace_all(tDocs,
c("\001" = "",
"\002" = "",
"\003" = "",
"\004" = "",
"\005" = "",
"\006" = ""
)
)
library(udpipe)
#model <- udpipe_download_model(language = "spanish-ancora") #Only first time to get the .udpipe file
#Alternativas: (1) spanish
#              (2) spanish-ancora
#fileWithPath <- paste0(getwd(), "/", params$model)
udmodel_spanish <- udpipe_load_model(file = params$model) #Lo cojo de la cabecera de Markdown
#Alternativas: (1) spanish-gsd-ud-2.3-181115.udpipe
#              (2) spanish-ancora-ud-2.3-181115.udpipe
s <- udpipe_annotate(udmodel_spanish,
#tokenizer = "tokenizer". El que tiene udpipes por defecto. Otras opciones en http://ufal.mff.cuni.cz/udpipe/users-manual
#tagger = "default".  El POS taggin y lematización por defecto de udpipes. También puede ser "none" o lo que dice en http://ufal.mff.cuni.cz/udpipe/users-manual
#parser = "default". El dependency parsing que tiene udpipes por defecto. También puede ser "none" o lo que dice en http://ufal.mff.cuni.cz/udpipe/users-manual
#trace = TRUE,#Por defecto es FALSE. Muestra el progreso de la anotación.
t2) #Tarda nada
#Se podría PARALELIZAR. Mira https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-parallel.html
x <- data.frame(s) #Tarda unos minutos
#save(x, file="x.df.udpipeanno.termcat.es.saved") #Lo cargo con load("x.df.udpipeanno.saved")
#load("x.df.udpipeanno.termcat.es.saved")
#Plotting Part-of-speech tags from the given text
library(lattice)
stats <- txt_freq(x$upos) #upos = universal part of speech
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = stats, col = "yellow",
main = "UPOS (Universal Parts of Speech)\n frequency of occurrence",
xlab = "Freq")
stats <- subset(x, upos %in% c("VERB"))
stats <- txt_freq(stats$token)# también puedo poner lemma si quiero ver el verbo en infinitivo
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "gold",
main = "Most occurring Verbs (token)", xlab = "Freq")
stats <- subset(x, upos %in% c("VERB"))
stats <- txt_freq(stats$lemma)# el lemma para el verbo en infinitivo
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "light blue",
main = "Most occurring Verbs (lemma)", xlab = "Freq")
x$phrase_tag <- as_phrasemachine(x$upos,
type = "upos" #Puede ser también "penn-treebank"
)#Convierte los tags de upos a phrasemachine (Handler 2016).
df_doc_token     <- aggregate(token ~ doc_id, data = x, paste, collapse = " ")
df_doc_phrase_tag <- aggregate(phrase_tag ~ doc_id, data = x, paste, collapse = "")
df_doc_token_phrasetag <- df_doc_token
df_doc_token_phrasetag$phrase_tag <- df_doc_phrase_tag$phrase_tag
tab <- table(df_doc_token_phrasetag$phrase_tag)
#Ordeno la tabla de contingencia
tab_ord <- sort(tab, decreasing = T)
#Lo que más hay es NA (216), luego N (141)....
library(knitr)
kable (tab_ord, caption = "Número de casos de cada POS pattern") #Muestro todos
barplot(tab_ord,
log="y", #Log scale in y axis
main="POS patterns",
ylab="Number of instances (log scale)",
xlab="POS pattern",
las=2 #Rotated lables in the x axis
)
barplot(cumsum(tab_ord)/nrow(df_doc_token_phrasetag),
#log="y", #Log scale in y axis
main="POS patterns (Cumulative Sum)",
ylab="Percentage covered",
xlab="POS patterns",
las=2 #Rotated lables in the x axis
)
library(ggplot2)
tabsum <- cumsum(tab_ord)/nrow(df_doc_token_phrasetag)
cumdf <- data.frame(cum = tabsum,
stringsAsFactors = FALSE) #Tiene una única col
cumdf$pat <- rownames(cumdf) #Nueva col con pattens
top <- 12
defcolor <- "#EEEEEE"  #light gray para imprimir (casi no se ve en pantalla). Si uso #BBBBBB sale muy oscuro impreso
#Un tono más oscuro es #CCCCCC . Si uso #777777 sale oscurito en pantalla pero muy oscuro impreso
micolor <- c(rep("#CCCCCC", top), rep(defcolor, nrow(cumdf) - top)) #Vector de colores
sumtop <- cumdf$cum[top] #Suma de los 12 primeros
ggplot(data=cumdf,
aes(x=pat, y=cum, fill = pat)) +
scale_x_discrete(limits = cumdf$pat) + #Sin esto ordena alfabéticamente
scale_y_continuous(labels = scales::percent_format(accuracy = 1),  #Que ponga porcentaje (en lugar de float) en eje y
breaks = seq(0,1,0.1), #Que pinte estos (de 0 a 1 (100%) de 0.1 (10%) en 10%)
expand = expand_scale(mult = c(0, 0)) #Elimina espacios extra por debajo y por arriba, para que vaya justo de 0 a 1 (100%).
) +
labs( #title = "The title",
#subtitle = "the subtitle",        #will be displayed below the title.
#caption = "Some ref of copyright" #The text for the caption which will be displayed in the bottom-right of the plot by default.
#tag = "some letter useful "
x = "POS patterns ordered (most used in the left)", #Label of the x axis
y = "Percentage of entities covered (cummulative)" #Label of the x axis
) +
geom_bar(stat="identity",
show.legend = FALSE, #Avoids leyend (big, one class per color)
#color = "#111111", #Color del borde. Si no lo pongo no poner borde
#fill = defcolor   #Color del relleno
) +
scale_fill_manual (values = micolor, #coloreo distinto EL INTERIOR de las top barras
limits = cumdf$pat #Es imprescindible si estoy forzando el orden
) +
geom_segment(aes(x=cumdf$pat[1],  y=sumtop  , xend=cumdf$pat[top], yend=sumtop),   #Add a horizontal segment (line)
size = 0.5, #Grosor de la línea. Por def = 1
color="red") +
geom_segment(aes(x=cumdf$pat[top], y=0.0,      xend=cumdf$pat[top], yend=sumtop),   #Add a vertical segment (line)
size = 0.5, #Grosor de la línea. Por def = 1
color="red") +
annotate(geom="text",
x=cumdf$pat[1],
y=sumtop + 0.05, #A litte above the line
hjust = 0, #La posición será la del inicio de la cadena
label=paste0("The ", top, " most used patterns cover ",
round(100* max(cumsum(tab_ord[1:top])/nrow(df_doc_token_phrasetag)), #Top x cumsum
digits=0 #Sin decimales
),
"% of the terms"
),
color="red") +
theme_classic() + #Light es un theme algo más mono que bw o classic pero que impreso no queda bien
theme(#element_text(size=20),                            #El tamaño del font general
axis.text.x = element_text(angle = 90,#Para girar las labels del eje x
#size = rel(0.95), # puedo poner un vaor absoluto (e.g. 10) o relativo
vjust = 0.5, #0 means left-justified, 1 means right-justified.
hjust = 1    #0 means left-justified, 1 means right-justified.
)
)
#Veamos un caso
library(knitr)
#Sale que con NPNPN hay 18 términos
ejemplosPat <- function(pat){
df_doc_token_phrasetag[df_doc_token_phrasetag$phrase_tag == pat, ]
} #retorna un data.frame
kable (ejemplosPat("NPNPN"), caption = "Todos los términos con el POS pattern NPNPN") #Muestro todos
#Los ejemplos de pattern ("NA, "AN", "NPNA", etc...) hasta un máximo de 20
tablaEjemplosPat <- function(pat){
ej <- ejemplosPat(pat)
num <- min(20, nrow(ej))
kable (ej[c(1:num),], caption = paste (num, "términos con el POS pattern", pat))
}
# tablaEjemplosPat("NA")
# tablaEjemplosPat("N")
# tablaEjemplosPat("NPN")
# tablaEjemplosPat("V")
# tablaEjemplosPat("NPNA")
# tablaEjemplosPat("NPDN")
# tablaEjemplosPat("A")
# tablaEjemplosPat("AN")
# tablaEjemplosPat("NN")
# tablaEjemplosPat("NPNPDNPNAA")
# tablaEjemplosPat("NOPDNAO")
# tablaEjemplosPat("O")
# tablaEjemplosPat("NPNOPDNAO")
# tablaEjemplosPat("NAA")
# tablaEjemplosPat("NAPN")
for  (pat in cumdf$pat){
tablaEjemplosPat(pat)
}
shiny::runApp()
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
setwd("~/Desktop")
read_csv("/extract_terms_user_pedrohv_abstracts_20200624111727.csv")
setwd("~/Desktop")
read_csv("/extract_terms_user_pedrohv_abstracts_20200624111727.csv")
setwd("~/Desktop")
read_csv("/extract_terms_user_pedrohv_abstracts_20200624111727.csv")
read_csv("extract_terms_user_pedrohv_abstracts_20200624111727.csv")
read_csv("extract_terms_user_pedrohv_abstracts_20200624111727.csv", sep = ';')
read_csv("extract_terms_user_pedrohv_abstracts_20200624111727.csv", sep = ';')
setwd("~/Desktop")
read_csv("extract_terms_user_pedrohv_abstracts_20200624111727.csv", sep = ';')
read.csv("extract_terms_user_pedrohv_abstracts_20200624111727.csv", sep = ';')
read.csv("extract_terms_user_pedrohv_abstracts_20200624111727.csv", sep = ';') -> list
View(list)
save.csv(list, "extractor", sep = ',')
write.csv(list, "extractor", sep = ',')
runApp('~/TFM/TerminologiasInteractivas')
shiny::runApp()
shiny::runApp()
runApp()
runApp()
shiny::runApp()
shiny::runApp()
runApp()
runApp()
runApp('~/InnoSpace/R/webapp')
runApp()
shiny::runApp()
runApp()
shiny::runApp()
runApp()
library(pbapple)
library(pbapply)
install.packages('pbapply')
runApp()
runApp()
runApp()
